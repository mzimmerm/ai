* TL;DR for this project, once everything is installed

jupyter-lab train-bert-on-yelp-local.ipynb : Start notebook


* Running Huggingface train for Bert with Yelp dataset

- Go to https://huggingface.co/docs/transformers/training

- Click "Open in Colab", select "mixed". Colab 

- This opens the whole article in Google Colab. Each piece of code is a cell.

- The cell sections are as follows:

  - Top: 'pip install transformers datasets'
    - Only this line runs
  - Fine-tune a pretrained model
    1. Prepare a dataset
    2. Train (no code)
    3. Train with PyTorch Trainer
    4. Train a TensorFlow model with Keras
    5. Train in native PyTorch

  - We will ONLY run 1,3: 'Prepare a dataset', and 'Train with Pytorch Trainer'

    1. Prepare a dataset
       - The 'tokenize' section takes about 10 minutes
         #+begin_src python
           tokenized_datasets = dataset.map(tokenize_function, batched=True)
         #+end_src

       - The 'tokenized_datasets' instance is DatasetDict, contains 2 Dataset instances, probably large
       - Small datasets:
         #+begin_src python
           small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
           small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
           print(small_eval_dataset)
           print(small_eval_dataset.data[0][1:3]) # label (0-5)
           print(small_eval_dataset.data[1][1:3]) # text of review
         #+end_src
         - Result:
           #+begin_example
             small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
             small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
             print(small_eval_dataset)
             print(small_eval_dataset.data[0][1:3]) # label (0-5)
             print(small_eval_dataset.data[1][1:3]) # text of review

           #+end_example
         - The 'small_train_dataset' and 'small_eval_dataset' instances are of type Dataset

    2. Train with PyTorch Trainer
       - Import Model class and create model, defining only the features from above.
         #+begin_src python
           from transformers import AutoModelForSequenceClassification

           model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5) # labels correspond to 5 Dataset features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],
         #+end_src
       - The 'model' instance has tensors, about 400M! This must be the Bert model already trained, that is why so large
       - Training hyperparameters
         - This section uses default 'hyperparameters'


* Running Locally

** For local runnning, I use python virtual environment ~/software/python/venv3.11~

- cd this directory
- source \~/software/python/venv3.11/bin/activate
- 

           

* Useful linux and python commands for AI

** Synchronize

On server, synchronize the 'ai' directory to laptop
# rsync the "ai" directory from server to laptop
# remove the --dry-run

rsync --dry-run --verbose --mkpath --archive /home/mzimmermann/dev/my-projects-source/public-on-github/ai mzimmermann@acer-ryzen-laptop-wifi:~/dev/my-projects-source/public-on-github


** Find and change VRAM used by GPU

# Find memory (vram) aveilable to AMG GPU
lspci -D # Find line that looks like graphics, e.g. "0000:03:00.0 VGA
compatible controller: ..."
# Then run 
cat /sys/bus/pci/devices/0000:03:00.0/mem_info_vram_total # Shows total VRAM size

# Another way
glxinfo | egrep -i 'device|memory'

AFTER rocm install:
localhost:~ # glxinfo | egrep -i 'device|memory'
    Device: llvmpipe (LLVM 15.0.7, 256 bits) (0xffffffff)
    Video memory: 14875MB
    Unified memory: yes
    GL_AMD_multi_draw_indirect, GL_AMD_pinned_memory, 
    GL_EXT_framebuffer_object, GL_EXT_framebuffer_sRGB, GL_EXT_memory_object, 
    GL_EXT_memory_object_fd, GL_EXT_packed_depth_stencil, GL_EXT_packed_float, 
    GL_AMD_pinned_memory, GL_AMD_query_buffer_object, 
    GL_EXT_gpu_program_parameters, GL_EXT_gpu_shader4, GL_EXT_memory_object, 
    GL_EXT_memory_object_fd, GL_EXT_multi_draw_arrays, 
    GL_EXT_map_buffer_range, GL_EXT_memory_object, GL_EXT_memory_object_fd, 

See this topic:

https://bbs.archlinux.org/viewtopic.php?id=283308

which explains to use AMD tool to change VRAM in BIOS. BUT with ROCm
installed, it appears all memory is available for video
  
  



* Install ROCm on Linux, for AMD Ryzen APUs

I have AMD APU Ryzen 5 2500U.

This text is only concerned with Linux running AMD software for iGPU (GPU
integrated in APU).

AMD was late in software support for their APUs and GPUs to run machine
learning (ML) training. Simplifying, we can say that the Python PyTorch
package is the standard way to run ML algorithms. While PyTorch can run on
both CPU and GPU, only GPU processing is reasonably performant. GPU processing
needs support from low level software on the GPU. AMD's GPU low level software
is ROCm. todo: how does AMD describe ROCm? ROCm, officially supports a 
miniscule number of discrete video cards; it does not officially support any
iGPU, see
https://rocm.docs.amd.com/projects/radeon/en/latest/docs/compatibility.html. 
A bug report to support ROCm on 4-year old Renoir GPUs is closed with
(paraphrasing) "not supported, some have success running it", see
https://github.com/ROCm/ROCm/issues/1101. Not exactly a vote of confidence.


** Installing ROCm on Linux

Presumably (obviously) one does have to install ROCm software and drivers on their system to run ML
efficiently. AMD's documentation of is confusing, it is not clear where to start, we have

1. https://rocm.docs.amd.com/projects/radeon/en/latest/index.html
2. https://rocm.docs.amd.com/en/latest/
3. https://www.amd.com/en/support/linux-drivers
4. https://community.amd.com/t5/ai/amd-extends-support-for-pytorch-machine-learning-development-on/ba-p/637756
5. todo: add lmstudio links, explain differences.


Look-ahead note: After looking into the instructions, there are two distinct
elements: Something called "amdgpu" and "rocm". It appears that AMD uses the
term "amdgpu" when it refers to the whole package. "rocm" seems to be the name
of the kernel driver, so it is a "part of" the "amdgpu" package.

Deeper in the second link, we find 
https://rocm.docs.amd.com/projects/install-on-linux/en/latest/tutorial/quick-start.html
which offers two methods of installing ROCm on Linux.

- Native package manager
- AMDGPU installer

Native package manager is a Linux-distro specific package, offered for Debian,
RedHat, and SuSE's SLE.

AMDGPU installer is also a Linux-distro specific package, offered for the same
distros.

The installer needs to be executed, the native package installation requires a
few more manual steps to install the driver.  In line with the AMD's ROCm
confusion, it is not clear why there are two methods, which is better or
preferred and what are the differences between them.

In my installation, I am using the "Native package manager" approach on SuSE Leap 15.5. I know, Leap !=
SLE, but they should be interchangeable, and the process worked, as you can
see if you keep reading. These are the installation steps from https://rocm.docs.amd.com/projects/install-on-linux/en/latest/tutorial/quick-start.html#suse-linux-enterprise-server:

----
sudo zypper addrepo https://download.opensuse.org/repositories/devel:languages:perl/SLE_15_SP5/devel:languages:perl.repo
sudo zypper install kernel-default-devel
# See prerequisites. Adding current user to Video and Render groups
sudo usermod -a -G render,video $LOGNAME
sudo zypper --no-gpg-checks install https://repo.radeon.com/amdgpu-install/6.0.2/sle/15.5/amdgpu-install-6.0.60002-1.noarch.rpm
sudo zypper refresh
sudo zypper install amdgpu-dkms
sudo zypper install rocm
echo "Please reboot system for all settings to take effect."
----

The above steps ran with no error on OpenSUSe Leap 15.5.

Rebooted the system.

todo improve: Added latest-2,latest-3 to /etc/zypp/zypp.conf


** Validating ROCm installation

After the above installation of ROCm (amdgpu) todo check if it worked and how


AFTER rocm install:
localhost:~ # glxinfo | egrep -i 'device|memory'
    Device: llvmpipe (LLVM 15.0.7, 256 bits) (0xffffffff)
    Video memory: 14875MB
    Unified memory: yes
    GL_AMD_multi_draw_indirect, GL_AMD_pinned_memory, 
    GL_EXT_framebuffer_object, GL_EXT_framebuffer_sRGB, GL_EXT_memory_object, 
    GL_EXT_memory_object_fd, GL_EXT_packed_depth_stencil, GL_EXT_packed_float, 
    GL_AMD_pinned_memory, GL_AMD_query_buffer_object, 
    GL_EXT_gpu_program_parameters, GL_EXT_gpu_shader4, GL_EXT_memory_object, 
    GL_EXT_memory_object_fd, GL_EXT_multi_draw_arrays, 
    GL_EXT_map_buffer_range, GL_EXT_memory_object, GL_EXT_memory_object_fd, 

it appears all memory is available for video

BUT
> rocminfo
ROCk module is NOT loaded, possibly no GPU devices

So it appears rocm was not installed?? todo what does it mean??

#> dmesg | grep kfd
no output

#> dmesg | grep rocm
no output

So I did:

zypper in amdgpu # this is not in instructions todo- explain.

But it did not help

mv /etc/modprobe.d/blacklist-radeon.conf ~/tmp

modprobe amdgpu
modprobe: ERROR: could not insert 'amdgpu': Key was rejected by service

todo: this seems important:

localhost:/etc/modprobe.d # modprobe rocm
modprobe: FATAL: Module rocm not found in directory /lib/modules/5.14.21-150500.55.52-default

#> sudo modinfo amdgpu
shows some massive amount of stuff. maybe this is a path??

als

#> sudo modprobe -vvr amdgpu

#> mzimmermann@localhost:~/tmp> sudo modprobe -vv amdgpu
modprobe: INFO: custom logging function 0x55a45ae0d260 registered
insmod /lib/modules/5.14.21-150500.55.52-default/updates/amdkcl.ko.zst 
modprobe: INFO: Failed to insert module '/lib/modules/5.14.21-150500.55.52-default/updates/amdkcl.ko.zst': Key was rejected by service
modprobe: ERROR: could not insert 'amdgpu': Key was rejected by service
modprobe: INFO: context 0x55a45b3ad440 released


so going after amdkcl.ko
this has someting to do with secure boot - should be disabled

sudo mokutil --sb-state
SecureBoot enabled

So try to disable it???

From https://github.com/linux-surface/linux-surface/issues/906 it looks like
installing ssl could install a secure boot key, which fixes the problem??

According to the DKMS github page, on most distro, the key pair is located at /var/lib/dkms/[mok.key & mok.pub]. If the file is not present, it will generate one using openssl package.

On Ubuntu though, it will be configured to use the Ubuntu master key, which is
located in: /var/lib/shim-signed/mok/MOK.der and
/var/lib/shim-signed/mok/MOK.priv which is generated and enrolled when you
first install Ubuntu, where it will automatically configure SecureBoot.

So I did:

Yast->bootloader, uncheck "secure boot support"
reboot
THE ABOVE ALMOST FIXED IT, BUT SYSTEM DID NOT BOOT. I HAD TO ADD A PASSWORD TO
BIOS, THEN DISABLE SECURE BOOT IN BIOS.

AFTER REBOOT, rocm and amdgpu loaded:

localhost:~ # rocminfo | grep gfx
  Name:                    gfx902                             
      Name:                    amdgcn-amd-amdhsa--gfx902:xnack+   


localhost:~ # modprobe -vv amdgpu
modprobe: INFO: custom logging function 0x5604f440d260 registered
modprobe: INFO: context 0x5604f5db1690 released


Running a script that tests everything rocm. The script is mentioned in
https://github.com/ROCm/ROCm/issues/2216 and is here:
https://gist.github.com/damico/484f7b0a148a0c5f707054cf9c0a0533

Save the script as, for example  misc/test-rocm.py and run


- source ~/software/python/venv3.6-for-ai-rocm/bin/activate
- python misc/test-rocm.py

  (venv3.6-for-ai-rocm) mzimmermann@localhost:~/dev/my-projects-source/public-on-github/ai/transformers/llm/train-bert-on-yelp> python misc/test-rocm.py


Checking ROCM support...
GOOD: ROCM devices found:  2
Checking PyTorch...
GOOD: PyTorch is working fine.
Checking user groups...
GOOD: The user mzimmermann is in RENDER and VIDEO groups.
BAD: PyTorch ROCM support NOT found.

  
So ROCM is found, PyTorch works, but it does not have ROCm support.

That means , we have to install PyTorch that works with ROCm. This is done in
the next step

** Installing PyTorch that works with ROCm

Pytorch is a Python package for ML. As a reasonably complex Python package, it
will only work on certain Python versions. The default Python
of SLE 15 and OpenSuse Leap 15.5 is Python 3.6. So we have to install Pytorch
that works with Python 3.6. There is a site which allows to
select operating system, language, package, and platform (CUDA or ROCm), and
download PyTorch for the selected version. The site is
https://pytorch.org/get-started/locally/.

file:./misc/pytorch-installation.png

Problem is, it says it only works on Python 3.8 or higher - while SLE 15 and OpenSuse Leap 15.5 only support Python
3.6. How do we solve this??? todo

If we are not in venv, run this:

- source ~/software/python/venv3.6-for-ai-rocm/bin/activate
- then the command listed
- pip3 install --pre torch torchvision torchaudio --index-url
  https://download.pytorch.org/whl/nightly/rocm6.0
  
We get:

Looking in indexes: https://download.pytorch.org/whl/nightly/rocm6.0
Requirement already satisfied: torch in /home/mzimmermann/software/python/venv3.6-for-ai-rocm/lib/python3.6/site-packages (1.10.2)
ERROR: Could not find a version that satisfies the requirement torchvision (from versions: none)
ERROR: No matching distribution found for torchvision


For now, let's say we do NOT care about torchvision and torchaudio, we can
just install torch:

-  pip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/rocm6.0
(requirement already satisfied in my case)

So run the scripot again:
(venv3.6-for-ai-rocm) mzimmermann@localhost:~/dev/my-projects-source/public-on-github/ai/transformers/llm/train-bert-on-yelp> python misc/test-rocm.py


Checking ROCM support...
GOOD: ROCM devices found:  2
Checking PyTorch...
GOOD: PyTorch is working fine.
Checking user groups...
GOOD: The user mzimmermann is in RENDER and VIDEO groups.
BAD: PyTorch ROCM support NOT found.
(venv3.6-for-ai-rocm)
mzimmermann@localhost:~/dev/my-projects-source/public-on-github/ai/tran

SAME ERROR, WHY??

I will try to follow this solution:

https://github.com/pytorch/pytorch/issues/120433

RUN THIS:

- pip3 uninstall torch
- pip3 install --pre torch --index-url https://repo.radeon.com/rocm/manylinux/rocm-rel-6.0.2/torch-2.1.2+rocm6.0-cp310-cp310-linux_x86_64.whl

  ERROR

  Looking in indexes: https://repo.radeon.com/rocm/manylinux/rocm-rel-6.0.2/torch-2.1.2+rocm6.0-cp310-cp310-linux_x86_64.whl
ERROR: Could not find a version that satisfies the requirement torch (from versions: none)
ERROR: No matching distribution found for torch

SAME ERROR ON SERVER - I UNINSTALLED AND F***ED TORCH THERE AS WELL...

HOW TO FIX??

** todo Addendums

*** todo CUDA CUDA, which stands for Compute Unified Device Architecture, is a parallel computing platform and programming model developed by NVIDIA
