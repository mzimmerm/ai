* TL;DR for this project, once everything is installed

jupyter-lab train-bert-on-yelp-local.ipynb : Start notebook


* Running Huggingface train for Bert with Yelp dataset

- Go to https://huggingface.co/docs/transformers/training

- Click "Open in Colab", select "mixed". Colab 

- This opens the whole article in Google Colab. Each piece of code is a cell.

- The cell sections are as follows:

  - Top: 'pip install transformers datasets'
    - Only this line runs
  - Fine-tune a pretrained model
    1. Prepare a dataset
    2. Train (no code)
    3. Train with PyTorch Trainer
    4. Train a TensorFlow model with Keras
    5. Train in native PyTorch

  - We will ONLY run 1,3: 'Prepare a dataset', and 'Train with Pytorch Trainer'

    1. Prepare a dataset
       - The 'tokenize' section takes about 10 minutes
         #+begin_src python
           tokenized_datasets = dataset.map(tokenize_function, batched=True)
         #+end_src

       - The 'tokenized_datasets' instance is DatasetDict, contains 2 Dataset instances, probably large
       - Small datasets:
         #+begin_src python
           small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
           small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
           print(small_eval_dataset)
           print(small_eval_dataset.data[0][1:3]) # label (0-5)
           print(small_eval_dataset.data[1][1:3]) # text of review
         #+end_src
         - Result:
           #+begin_example
             small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
             small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
             print(small_eval_dataset)
             print(small_eval_dataset.data[0][1:3]) # label (0-5)
             print(small_eval_dataset.data[1][1:3]) # text of review

           #+end_example
         - The 'small_train_dataset' and 'small_eval_dataset' instances are of type Dataset

    2. Train with PyTorch Trainer
       - Import Model class and create model, defining only the features from above.
         #+begin_src python
           from transformers import AutoModelForSequenceClassification

           model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5) # labels correspond to 5 Dataset features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],
         #+end_src
       - The 'model' instance has tensors, about 400M! This must be the Bert model already trained, that is why so large
       - Training hyperparameters
         - This section uses default 'hyperparameters'


* Running Locally

** For local runnning, I use python virtual environment ~/software/python/venv3.11~

- cd this directory
- source \~/software/python/venv3.11/bin/activate
- 

           

* Useful linux and python commands for AI

** Synchronize

On server, synchronize the 'ai' directory to laptop
# rsync the "ai" directory from server to laptop
# remove the --dry-run

rsync --dry-run --verbose --mkpath --archive /home/mzimmermann/dev/my-projects-source/public-on-github/ai mzimmermann@acer-ryzen-laptop-wifi:~/dev/my-projects-source/public-on-github


** Find and change VRAM used by GPU

# Find memory (vram) aveilable to AMG GPU
lspci -D # Find line that looks like graphics, e.g. "0000:03:00.0 VGA compatible controller: ..."
cat /sys/bus/pci/devices/0000:03:00.0/mem_info_vram_total # Shows total VRAM size

# Another way
glxinfo | egrep -i 'device|memory'

See this topic:

https://bbs.archlinux.org/viewtopic.php?id=283308

which explains to use AMD tool to change VRAM in BIOS
  
  



* Install ROCm on Linux, for AMD Ryzen APUs

I have AMD APU Ryzen 5 2500U.

This text is only concerned with Linux running AMD software for iGPU (GPU
integrated in APU).

AMD was late in software support for their APUs and GPUs to run machine
learning (ML) training. Simplifying, we can say that the Python PyTorch
package is the standard way to run ML algorithms. While PyTorch can run on
both CPU and GPU, only GPU processing is reasonably performant. GPU processing
needs support from low level software on the GPU. AMD's GPU low level software
is ROCm. todo: how does AMD describe ROCm? ROCm, officially supports a 
miniscule number of discrete video cards; it does not officially support any
iGPU, see
https://rocm.docs.amd.com/projects/radeon/en/latest/docs/compatibility.html. 
A bug report to support ROCm on 4-year old Renoir GPUs is closed with
(paraphrasing) "not supported, some have success running it", see
https://github.com/ROCm/ROCm/issues/1101. Not exactly a vote of confidence.


Presumably (obviously) one does have to install ROCm software and drivers on their system to run ML
efficiently. AMD's documentation of is confusing, it is not clear where to start, we have

1. https://rocm.docs.amd.com/projects/radeon/en/latest/index.html
2. https://rocm.docs.amd.com/en/latest/
3. https://www.amd.com/en/support/linux-drivers
4. https://community.amd.com/t5/ai/amd-extends-support-for-pytorch-machine-learning-development-on/ba-p/637756
5. todo: add lmstudio links, explain differences.


Deeper in the second link,
https://rocm.docs.amd.com/projects/install-on-linux/en/latest/tutorial/quick-start.html
offers two methods of installing ROCm on Linux

- Native package manager
- AMDGPU installer

Native package manager is a Linux-distro specific package, offered for Debian,
RedHat, and SuSE's SLE.

AMDGPU installer is also a Linux-distro specific package, offered for the same
distros.

The installer needs to be executed, the native package installation requires a
few more manual steps to install the driver.  In line with the AMD's ROCm
confusion, it is not clear why there are two methods, which is better or
preferred and what are the differences.

I am using the "Native package manager" approach on SuSE Leap 15.5. I know, Leap !=
SLE, but here are the instructions:

----
sudo zypper addrepo https://download.opensuse.org/repositories/devel:languages:perl/SLE_15_SP5/devel:languages:perl.repo
sudo zypper install kernel-default-devel
# See prerequisites. Adding current user to Video and Render groups
sudo usermod -a -G render,video $LOGNAME
sudo zypper --no-gpg-checks install https://repo.radeon.com/amdgpu-install/6.0.2/sle/15.5/amdgpu-install-6.0.60002-1.noarch.rpm
sudo zypper refresh
sudo zypper install amdgpu-dkms
sudo zypper install rocm
echo "Please reboot system for all settings to take effect."
----

The above steps ran with no error on OpenSUSe Leap 15.5.

Rebooted the system.
