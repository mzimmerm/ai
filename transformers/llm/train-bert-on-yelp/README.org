* Ubuntu configuration changes during setup

gsettings set org.gnome.SessionManager auto-save-session true # save session across reboot

/home/mzimmermann/.local/bin # to instal glxinfo

* TL;DR for this project, once everything is installed

jupyter-lab train-bert-on-yelp-local.ipynb : Start notebook


* Running Huggingface train for Bert with Yelp dataset

- Go to https://huggingface.co/docs/transformers/training

- Click "Open in Colab", select "mixed". Colab 

- This opens the whole article in Google Colab. Each piece of code is a cell.

- The cell sections are as follows:

  - Top: 'pip install transformers datasets'
    - Only this line runs
  - Fine-tune a pretrained model
    1. Prepare a dataset
    2. Train (no code)
    3. Train with PyTorch Trainer
    4. Train a TensorFlow model with Keras
    5. Train in native PyTorch

  - We will ONLY run 1,3: 'Prepare a dataset', and 'Train with Pytorch Trainer'

    1. Prepare a dataset
       - The 'tokenize' section takes about 10 minutes
         #+begin_src python
           tokenized_datasets = dataset.map(tokenize_function, batched=True)
         #+end_src

       - The 'tokenized_datasets' instance is DatasetDict, contains 2 Dataset instances, probably large
       - Small datasets:
         #+begin_src python
           small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
           small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
           print(small_eval_dataset)
           print(small_eval_dataset.data[0][1:3]) # label (0-5)
           print(small_eval_dataset.data[1][1:3]) # text of review
         #+end_src
         - Result:
           #+begin_example
             small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
             small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
             print(small_eval_dataset)
             print(small_eval_dataset.data[0][1:3]) # label (0-5)
             print(small_eval_dataset.data[1][1:3]) # text of review

           #+end_example
         - The 'small_train_dataset' and 'small_eval_dataset' instances are of type Dataset

    2. Train with PyTorch Trainer
       - Import Model class and create model, defining only the features from above.
         #+begin_src python
           from transformers import AutoModelForSequenceClassification

           model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5) # labels correspond to 5 Dataset features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],
         #+end_src
       - The 'model' instance has tensors, about 400M! This must be the Bert model already trained, that is why so large
       - Training hyperparameters
         - This section uses default 'hyperparameters'


* Running Locally

** For local runnning, I use python virtual environment ~/software/python/venv3.11~

- cd this directory
- source \~/software/python/venv3.11/bin/activate
- 

           

* Useful linux and python3 commands for AI

** Synchronize

On server, synchronize the 'ai' directory to laptop
# rsync the "ai" directory from server to laptop
# remove the --dry-run

rsync --dry-run --verbose --mkpath --archive /home/mzimmermann/dev/my-projects-source/public-on-github/ai mzimmermann@acer-ryzen-laptop-wifi:~/dev/my-projects-source/public-on-github


** Find and change VRAM used by GPU

# Find memory (vram) aveilable to AMG GPU
lspci -D # Find line that looks like graphics, e.g. "0000:03:00.0 VGA
compatible controller: ..."
# Then run 
cat /sys/bus/pci/devices/0000:03:00.0/mem_info_vram_total # Shows total VRAM size

# Another way
glxinfo | egrep -i 'device|memory'

AFTER rocm install:
localhost:~ # glxinfo | egrep -i 'device|memory'
    Device: llvmpipe (LLVM 15.0.7, 256 bits) (0xffffffff)
    Video memory: 14875MB
    Unified memory: yes
    GL_AMD_multi_draw_indirect, GL_AMD_pinned_memory, 
    GL_EXT_framebuffer_object, GL_EXT_framebuffer_sRGB, GL_EXT_memory_object, 
    GL_EXT_memory_object_fd, GL_EXT_packed_depth_stencil, GL_EXT_packed_float, 
    GL_AMD_pinned_memory, GL_AMD_query_buffer_object, 
    GL_EXT_gpu_program_parameters, GL_EXT_gpu_shader4, GL_EXT_memory_object, 
    GL_EXT_memory_object_fd, GL_EXT_multi_draw_arrays, 
    GL_EXT_map_buffer_range, GL_EXT_memory_object, GL_EXT_memory_object_fd, 

See this topic:

https://bbs.archlinux.org/viewtopic.php?id=283308

which explains to use AMD tool to change VRAM in BIOS. BUT with ROCm
installed, it appears all memory is available for video
  
  



* todo Addendums to ROCm install

** todo CUDA CUDA, which stands for Compute Unified Device Architecture, is a parallel computing platform and programming model developed by NVIDIA

* Install ROCm on Linux, for AMD Ryzen APUs

I have AMD APU Ryzen 5 2500U. Lists graphics as  GCN 5th generation , same as the GT versions

This text is only concerned with Linux running AMD software for iGPU (GPU
integrated in APU).

AMD was late in software support for their APUs and GPUs to run machine
learning (ML) training. Simplifying, we can say that the Python PyTorch
package is the standard way to run ML algorithms. While PyTorch can run on
both CPU and GPU, only GPU processing is reasonably performant. GPU processing
needs support from low level software on the GPU. AMD's GPU low level software
is ROCm. todo: how does AMD describe ROCm? ROCm, officially supports a 
miniscule number of discrete video cards; it does not officially support any
iGPU, see
https://rocm.docs.amd.com/projects/radeon/en/latest/docs/compatibility.html. 
A bug report to support ROCm on 4-year old Renoir GPUs is closed with
(paraphrasing) "not supported, some have success running it", see
https://github.com/ROCm/ROCm/issues/1101. Not exactly a vote of confidence.


** Install ROCm on Linux

Presumably (obviously) one does have to install ROCm software and drivers on their system to run ML
efficiently. AMD's documentation of is confusing, it is not clear where to start, we have

1. https://rocm.docs.amd.com/projects/radeon/en/latest/index.html
2. https://rocm.docs.amd.com/en/latest/
3. https://www.amd.com/en/support/linux-drivers
4. https://community.amd.com/t5/ai/amd-extends-support-for-pytorch-machine-learning-development-on/ba-p/637756
5. todo: add lmstudio links, explain differences.


Look-ahead note: After looking into the instructions, there are two distinct
elements: Something called "amdgpu" and "rocm". It appears that AMD uses the
term "amdgpu" when it refers to the whole package. "rocm" seems to be the name
of the kernel driver, so it is a "part of" the "amdgpu" package.

Deeper in the second link, we find 
https://rocm.docs.amd.com/projects/install-on-linux/en/latest/tutorial/quick-start.html
which offers two methods of install ROCm on Linux.

- Native package manager
- AMDGPU installer

Native package manager is a Linux-distro specific package, offered for Debian,
RedHat, and SuSE's SLE.

AMDGPU installer is also a Linux-distro specific package, offered for the same
distros.

The installer needs to be executed, the native package installation requires a
few more manual steps to install the driver.  In line with the AMD's ROCm
confusion, it is not clear why there are two methods, which is better or
preferred and what are the differences between them.

In my installation, I am using the "Native package manager" approach on SuSE Leap 15.5. I know, Leap !=
SLE, but they should be interchangeable, and the process worked, as you can
see if you keep reading. These are the installation steps from https://rocm.docs.amd.com/projects/install-on-linux/en/latest/tutorial/quick-start.html#suse-linux-enterprise-server:

----
sudo zypper addrepo https://download.opensuse.org/repositories/devel:languages:perl/SLE_15_SP5/devel:languages:perl.repo
sudo zypper install kernel-default-devel
# See prerequisites. Adding current user to Video and Render groups
sudo usermod -a -G render,video $LOGNAME
sudo zypper --no-gpg-checks install https://repo.radeon.com/amdgpu-install/6.0.2/sle/15.5/amdgpu-install-6.0.60002-1.noarch.rpm
sudo zypper refresh
sudo zypper install amdgpu-dkms
sudo zypper install rocm
echo "Please reboot system for all settings to take effect."
----

The above steps ran with no error on OpenSUSe Leap 15.5.

Rebooted the system.

todo improve: Added latest-2,latest-3 to /etc/zypp/zypp.conf


** Validating ROCm installation

After the above installation of ROCm (amdgpu) todo check if it worked and how


AFTER rocm install:
localhost:~ # glxinfo | egrep -i 'device|memory'
    Device: llvmpipe (LLVM 15.0.7, 256 bits) (0xffffffff)
    Video memory: 14875MB
    Unified memory: yes
    GL_AMD_multi_draw_indirect, GL_AMD_pinned_memory, 
    GL_EXT_framebuffer_object, GL_EXT_framebuffer_sRGB, GL_EXT_memory_object, 
    GL_EXT_memory_object_fd, GL_EXT_packed_depth_stencil, GL_EXT_packed_float, 
    GL_AMD_pinned_memory, GL_AMD_query_buffer_object, 
    GL_EXT_gpu_program_parameters, GL_EXT_gpu_shader4, GL_EXT_memory_object, 
    GL_EXT_memory_object_fd, GL_EXT_multi_draw_arrays, 
    GL_EXT_map_buffer_range, GL_EXT_memory_object, GL_EXT_memory_object_fd, 

it appears all memory is available for video

BUT
> rocminfo
ROCk module is NOT loaded, possibly no GPU devices

So it appears rocm was not installed?? todo what does it mean??

#> dmesg | grep kfd
no output

#> dmesg | grep rocm
no output

So I did:

zypper in amdgpu # this is not in instructions todo- explain.

But it did not help

mv /etc/modprobe.d/blacklist-radeon.conf ~/tmp

modprobe amdgpu
modprobe: ERROR: could not insert 'amdgpu': Key was rejected by service

todo: this seems important:

localhost:/etc/modprobe.d # modprobe rocm
modprobe: FATAL: Module rocm not found in directory /lib/modules/5.14.21-150500.55.52-default

#> sudo modinfo amdgpu
shows some massive amount of stuff. maybe this is a path??

als

#> sudo modprobe -vvr amdgpu

#> mzimmermann@localhost:~/tmp> sudo modprobe -vv amdgpu
modprobe: INFO: custom logging function 0x55a45ae0d260 registered
insmod /lib/modules/5.14.21-150500.55.52-default/updates/amdkcl.ko.zst 
modprobe: INFO: Failed to insert module '/lib/modules/5.14.21-150500.55.52-default/updates/amdkcl.ko.zst': Key was rejected by service
modprobe: ERROR: could not insert 'amdgpu': Key was rejected by service
modprobe: INFO: context 0x55a45b3ad440 released


so going after amdkcl.ko
this has someting to do with secure boot - should be disabled

sudo mokutil --sb-state
SecureBoot enabled

So try to disable it???

From https://github.com/linux-surface/linux-surface/issues/906 it looks like
installing ssl could install a secure boot key, which fixes the problem??

According to the DKMS github page, on most distro, the key pair is located at /var/lib/dkms/[mok.key & mok.pub]. If the file is not present, it will generate one using openssl package.

On Ubuntu though, it will be configured to use the Ubuntu master key, which is
located in: /var/lib/shim-signed/mok/MOK.der and
/var/lib/shim-signed/mok/MOK.priv which is generated and enrolled when you
first install Ubuntu, where it will automatically configure SecureBoot.

So I did:

Yast->bootloader, uncheck "secure boot support"
reboot
THE ABOVE ALMOST FIXED IT, BUT SYSTEM DID NOT BOOT. I HAD TO ADD A PASSWORD TO
BIOS, THEN DISABLE SECURE BOOT IN BIOS.

AFTER REBOOT, rocm and amdgpu loaded:

localhost:~ # rocminfo | grep gfx
  Name:                    gfx902                             
      Name:                    amdgcn-amd-amdhsa--gfx902:xnack+   


localhost:~ # modprobe -vv amdgpu
modprobe: INFO: custom logging function 0x5604f440d260 registered
modprobe: INFO: context 0x5604f5db1690 released


Running a script that tests everything rocm. The script is mentioned in
https://github.com/ROCm/ROCm/issues/2216 and is here:
https://gist.github.com/damico/484f7b0a148a0c5f707054cf9c0a0533

Save the script as, for example  misc/test-rocm.py and run


- source ~/software/python/venv3.6-for-ai-rocm/bin/activate
- python3 misc/test-rocm.py

  (venv3.6-for-ai-rocm) mzimmermann@localhost:~/dev/my-projects-source/public-on-github/ai/transformers/llm/train-bert-on-yelp> python3 misc/test-rocm.py


Checking ROCM support...
GOOD: ROCM devices found:  2
Checking PyTorch...
GOOD: PyTorch is working fine.
Checking user groups...
GOOD: The user mzimmermann is in RENDER and VIDEO groups.
BAD: PyTorch ROCM support NOT found.

  
So ROCM is found, PyTorch works, but it does not have ROCm support.

That means , we have to install PyTorch that works with ROCm. This is done in
the next step

** Install PyTorch that works with ROCm

Pytorch is a Python package for ML. As a reasonably complex Python package, it
will only work on certain Python versions. The default Python
of SLE 15 and OpenSuse Leap 15.5 is Python 3.6. So we have to install Pytorch
that works with Python 3.6. There is a site which allows to
select operating system, language, package, and platform (CUDA or ROCm), and
download PyTorch for the selected version. The site is
https://pytorch.org/get-started/locally/.

file:./misc/pytorch-installation.png

Problem is, it says it only works on Python 3.8 or higher - while SLE 15 and OpenSuse Leap 15.5 only support Python
3.6. How do we solve this??? todo

If we are not in venv, run this:

- source ~/software/python/venv3.6-for-ai-rocm/bin/activate
- then the command listed
- pip3 install --pre torch torchvision torchaudio --index-url
  https://download.pytorch.org/whl/nightly/rocm6.0
  
We get:

Looking in indexes: https://download.pytorch.org/whl/nightly/rocm6.0
Requirement already satisfied: torch in /home/mzimmermann/software/python/venv3.6-for-ai-rocm/lib/python3.6/site-packages (1.10.2)
ERROR: Could not find a version that satisfies the requirement torchvision (from versions: none)
ERROR: No matching distribution found for torchvision


For now, let's say we do NOT care about torchvision and torchaudio, we can
just install torch:

-  pip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/rocm6.0
(requirement already satisfied in my case)

So run the scripot again:
(venv3.6-for-ai-rocm) mzimmermann@localhost:~/dev/my-projects-source/public-on-github/ai/transformers/llm/train-bert-on-yelp> python3 misc/test-rocm.py


Checking ROCM support...
GOOD: ROCM devices found:  2
Checking PyTorch...
GOOD: PyTorch is working fine.
Checking user groups...
GOOD: The user mzimmermann is in RENDER and VIDEO groups.
BAD: PyTorch ROCM support NOT found.
(venv3.6-for-ai-rocm)
mzimmermann@localhost:~/dev/my-projects-source/public-on-github/ai/tran

SAME ERROR, WHY??

I will try to follow this solution:

https://github.com/pytorch/pytorch/issues/120433

RUN THIS:

- pip3 uninstall torch
- pip3 install --pre torch --index-url https://repo.radeon.com/rocm/manylinux/rocm-rel-6.0.2/torch-2.1.2+rocm6.0-cp310-cp310-linux_x86_64.whl

  ERROR

  Looking in indexes: https://repo.radeon.com/rocm/manylinux/rocm-rel-6.0.2/torch-2.1.2+rocm6.0-cp310-cp310-linux_x86_64.whl
ERROR: Could not find a version that satisfies the requirement torch (from versions: none)
ERROR: No matching distribution found for torch

SAME ERROR ON SERVER - I UNINSTALLED AND F***ED TORCH THERE AS WELL...

HOW TO FIX??

Trying this from https://rocm.docs.amd.com/_/downloads/radeon/en/latest/pdf/

- pip3 install --upgrade pip wheel

NOTE: During ~pip3 install some.whl~ file, we may get error: 'some.whl is not a supported wheel on this platform' it refers to the fact the WHL is cp310 for python 3.10 but our Python environment is 3.6.

1) wget
   https://repo.radeon.com/rocm/manylinux/rocm-rel-6.0.2/torchvision-0.16.1+rocm6.0-cp310-cp310-linux_x86_64.whl
2) wget https://repo.radeon.com/rocm/manylinux/rocm-rel-6.0.2/torch-2.1.2+rocm6.0-cp310-cp310-linux_x86_64.whl
3) pip3 install --force-reinstall torch-2.1.2+rocm6.0-cp310-cp310-linux_x86_64.whl
 ERROR: torch-2.1.2+rocm6.0-cp310-cp310-linux_x86_64.whl is not a supported wheel on this platform.

 The 'not supported wheel on this platform', refers to the fact the WHL is
   cp310 for python 3.10 but the environment is 3.6

   I NEED TO INSTALL PYENV ON LEAP, THEN INSTALL 3.10 INTO IT. SEE
   https://unix.stackexchange.com/questions/757039/install-additional-python-on-opensuse-without-breaking-existing-pythons


*** Software packages which allow to use any Python version

The above problem - AMD providing a Python package that requires a specific Python version (3.10, indicated by the *cp310* in the name) - is very common. We need Python 3.10, but is is not readily available on our system (Leap 15.5, but we would hit the same issure in Debian, RedHat etc).

There are in 3 Python related pieces of software which let us to work around the problem. See for example https://betterstack.com/community/questions/what-are-differences-between-python-virtual-environments/. All of them allow to create virtual environments that allow to either install and/or switch between Python versions. But each of the 3 has their pros and cons:

1. venv is Python build-in module
   - Pros: Build in Python, relatively simple to setup a virtual environment for multiple Python versions
   - Cons: It can only use and switch between Python versions available on the OS. If we need some really old, or really new Python version for which the OS doeas not have a system level package, we are out of luck. This is the case for us: There is no Python3.10 installable on Opensuse Leap 15.5, so we cannot use venv
2. pyvenv is Python script on top of venv.
   - Pros: As above
   - Cons: As above
3. pyenv is a OS-level thing. 
   - Pros:
     - It can install a massive variety of multiple Python versions, literally hundreds, pretty much any Python version and flavour that ever existed
     - The multiple versions are installed in userspace (user home directory)
     - User can switch between the versions globally (per user), locally (per directory), or for shell (per current shell lifetime)
   - Cons:
     - It must build the needed version from source. That means, various "devel" packages must be installed on our OS. Which packages? That depends on the OS. For Opensuse, RedHat, and Debian, see for example https://realpython.com/intro-to-pyenv/#build-dependencies
     - Corollary of the above, the Python version must be buildable on the OS. 

       
*** Using *pyenv* to install the Python version for which AMD provides PyTorch on ROCm (3.10 at the time of writing, March 2024)

Because ther is no official Python3.10 package for our OS (Opensuse leap 15.5), our only choice is using *pyenv*.

Let us list the status of Python on my system before installation:

#+BEGIN_SRC bash :results raw
zypper search --installed-only python | grep -v "-"
#+END_SRC

| Name             | Summary                                   |
| libpython3_6m1_0 | Python Interpreter shared library package |
| python3          | Python 3 Interpreter package              |

There is only python3, python3.6, python3.6m, all linked to python3.6

#+BEGIN_SRC bash :results raw
python3 --version
#+END_SRC

#+RESULTS:
Python 3.6.15

Now let us go ahead and install pyenv

#+BEGIN_SRC bash :results raw
sudo zypper install pyenv
#+END_SRC

For curiosity, list the versions pyenv can install for us. The list is massive, we just shoe a subset:

#+BEGIN_SRC bash
pyenv install --list
#+END_SRC

#+RESULTS:
|       Available |
|-----------------|
|          3.10.0 |
|        3.10-dev |
|          3.10.1 |
|          3.10.7 |
|       3.11.0rc2 |
|  anaconda-1.4.0 |
| stackless-3.7.5 |

The list in the result of 'pyenv install --list' comes from official Python releases and other sources.

Now we need to select the version we want to use for Pytorch-on-ROCm. In that, we are limited to the version(s) for which AMD builds and tests their Pytorch-on-ROCm. These versions are available in todo

AMD provides us with PyTorch build for Python 3.10. Let's use pyenv to install Python3.10.7. First we need to set some pyenv-used environment variables:

#+BEGIN_SRC sh
  bashrcFilename=~/.bash_profile  # Avoid potential bash issueof loop in eval. Prefer this if using bash.
  # Alternatives of the above
  # bashrcFilename=~/.bashrc      # For non-bash users
  # bashrcFilename=/etc/profile.local # needs sudo
  # bashrcFilename=/etc/bash.bashrc.local # needs sudo

  echo 'export PYENV_ROOT="$HOME/.pyenv"' >> $bashrcFilename
  echo 'command -v pyenv >/dev/null || export PATH="$PYENV_ROOT/bin:$PATH"' >> $bashrcFilename
  echo 'eval "$(pyenv init -)"' >> $bashrcFilename
#+END_SRC

*Logout and log back in to enforce running the profile file during login shell start*, as we changed profile above. (We only need to start a new konsole if we changed bashrc)

Note: The ~pyenv init -~ which runs at the login shell, or startup of a new shell, establishes the directory ~$HOME/.pyenv~ with pyenv shims and versions. So after logging out and in, $PYENV_ROOT is set to ~$HOME/.pyenv~. 

Having initialized pyenv, we are ready to start installing Python versions that may not be part of the system. They will be compiled and installed to the  ~$HOME/.pyenv/versions directory. We will install Python3.10.7 as follows:

#+BEGIN_SRC sh :results raw
  pyenv install 3.10.7
#+END_SRC

There are errors and warnings from the above install, such as
#+BEGIN_EXAMPLE
  ERROR: The Python ssl extension was not compiled. Missing the OpenSSL lib?
#+END_EXAMPLE

The reason is, the system is missing some development packages. This is where we pay the price for pyenv ability to install any Python version - we have to "know" what system packages to install for the the ~pyenv install~ to work, and add those packages. This will differ from system to system. On Opensuse Leap 15.5, we need to install the following:

#+BEGIN_SRC bash
sudo zypper in zlib-devel bzip2 libbz2-devel libffi-devel libopenssl-devel readline-devel sqlite3 sqlite3-devel xz xz-devel
#+END_SRC

Now run the install again

#+BEGIN_SRC sh :results raw
  pyenv install 3.10.7
#+END_SRC

it should succeed, and install Python to ~$HOME/.pyenv/versions/3.10.7/~.

Now, when we run
#+BEGIN_SRC sh :results raw
  which python3
  python3 --version
#+END_SRC

#+RESULTS:
/home/mzimmermann/.pyenv/shims/python3
Python 3.6.15

We can see that python3 is still the system version, which is good.

Having the desired version installed, pyenv allows 3 ways of using the installed version3.10.7: "shell", "local", "global" - see https://github.com/pyenv/pyenv?tab=readme-ov-file#switch-between-python-versions.

In brief, to setup our command line to use the version, we can use one of the three commands.
#+BEGIN_SRC sh
pyenv shell 3.10.7  # select just for current shell session
pyenv local 3.10.7  # select when you are in the current directory (or subdirectories)
pyenv global 3.10.7 #  select globally for your user account
#+END_SRC

In our use of a ML project that uses PyTorch on ROCm, we will want to create a directory, say ~$ML_PROJECT~ in which our code will run using Python3.10.7. The following is a one-time command we need to run for any commands from ~$ML_PROJECT~ use Python3.10.7 in the future:

#+BEGIN_SRC sh :results raw
  ML_PROJECT=$HOME/dev/my-projects-source/public-on-github/ai/transformers/llm/train-bert-on-yelp
  cd $ML_PROJECT
  pyenv local 3.10.7
#+END_SRC

To confirm that is indeed true, try this
#+BEGIN_SRC sh :results raw
  ML_PROJECT=$HOME/dev/my-projects-source/public-on-github/ai/transformers/llm/train-bert-on-yelp
  echo When in the directory for which we ran 'pyenv local 3.10.7', the 3.10.7 is used
  cd $ML_PROJECT
  which python3     # From $HOME/.pyenv/shims
  which python      # As above
  which pip3        # As above
  which pip         # As above
  python3 --version # 3.10.7, the pyenv version
  python --version  # as above
  pip3 --version     # 22.2.2, the pyenv version
  pip --version
  echo When in the directory above, the OS version is used
  cd ..
  which python3     # From $HOME/.pyenv/shims, but directed to OS version, see below
  python3 --version # 3.6.15, the OS version
  python --version  # no python
  which pip3        # From $HOME/.pyenv/shims, but directed to OS version, see below
  pip3 --version    # 20.0.2, the OS version
#+END_SRC

#+RESULTS:
When in the directory for which we ran pyenv local 3.10.7, the 3.10.7 is used
/home/mzimmermann/.pyenv/shims/python3
/home/mzimmermann/.pyenv/shims/python
/home/mzimmermann/.pyenv/shims/pip3
/home/mzimmermann/.pyenv/shims/pip
Python 3.10.7
Python 3.10.7
pip 22.2.2 from /home/mzimmermann/.pyenv/versions/3.10.7/lib/python3.10/site-packages/pip (python 3.10)
pip 22.2.2 from /home/mzimmermann/.pyenv/versions/3.10.7/lib/python3.10/site-packages/pip (python 3.10)
When in the directory above, the OS version is used
/home/mzimmermann/.pyenv/shims/python3
Python 3.6.15
/home/mzimmermann/.pyenv/shims/pip3
pip 20.0.2 from /usr/lib/python3.6/site-packages/pip (python 3.6)

The functionality of using the intended Python version when in the directory ~$ML_PROJECT~ is achieved by the presence of the file ~.python-version~ in the directory, so do not delete the file.



*** Install AMD's PyTorch-on-ROCm - FAILING: HIP error: shared object initialization failed

Now we are ready to install AMD's PyTorch-on-ROCm, using the AMD provided builds, as described in https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/install-pytorch.html, with additional motivation of WHY to use only the AMD builds of PyTorch discussed in https://github.com/pytorch/pytorch/issues/120433. 


RUN
#+BEGIN_SRC sh :results raw

  # This directory was setup to use pyenv Python3.10.7
  ML_PROJECT=$HOME/dev/my-projects-source/public-on-github/ai/transformers/llm/train-bert-on-yelp
  cd $ML_PROJECT

  # In case non-AMD versions are installed. Probably not needed with force-reinstall
  pip3 uninstall torch torchvision

  # Download AMD's PyTorch-on-ROCm
  wget https://repo.radeon.com/rocm/manylinux/rocm-rel-6.0.2/torch-2.1.2+rocm6.0-cp310-cp310-linux_x86_64.whl
  wget https://repo.radeon.com/rocm/manylinux/rocm-rel-6.0.2/torchvision-0.16.1+rocm6.0-cp310-cp310-linux_x86_64.whl

  # Force reinstall torch and torchvision
  PYTORCH_ROCM_ARCH=gfx900 USE_ROCM=1 MAX_JOBS=4 pip3 install --force-reinstall torch-2.1.2+rocm6.0-cp310-cp310-linux_x86_64.whl torchvision-0.16.1+rocm6.0-cp310-cp310-linux_x86_64.whl

#+END_SRC

The ~pip3 install~ installed torch, torchvision, and many packages they depend on to the pyenv directory ~$HOME/.pyenv/versions/3.10.7/lib/python3.10/site-packages~.

Now we can test if the installed PyTorch (refered above as PyTorch-on-ROMm) actually uses the GPU. If it does, a simple Python code should respond "True" to CUDA being available. NOTE THAT CUDA DOES NOT REFER TO NVIDIA, IT MERELY STATES THAT A LOW LEVEL GPU LIBRARY (ROCm IN OUR CASE) IS AVAILABLE TO PYTORCH.

#+BEGIN_SRC sh

  # This directory was setup to use pyenv Python3.10.7
  ML_PROJECT=$HOME/dev/my-projects-source/public-on-github/ai/transformers/llm/train-bert-on-yelp
  cd $ML_PROJECT
  
  python3 << EOF
  import torch
  print(torch.cuda.is_available())
  EOF
#+END_SRC

#+RESULTS:
: True

For completeness, run a script that tests both rocm and pytorch installation and running. The script is mentioned in https://github.com/ROCm/ROCm/issues/2216 and is here: https://gist.github.com/damico/484f7b0a148a0c5f707054cf9c0a0533

#+BEGIN_SRC sh

  # This directory was setup to use pyenv Python3.10.7
  ML_PROJECT=$HOME/dev/my-projects-source/public-on-github/ai/transformers/llm/train-bert-on-yelp
  cd $ML_PROJECT
  
  python3 ./misc/test-rocm.py
#+END_SRC

#+RESULTS:




* Script to fully uninstall and reinstall ROCm (amdgpu) and PyTorch

sudo amdgpu-install --uninstall
sudo zypper removerepo "amdgpu"
sudo zypper removerepo "amdgpu-src"
sudo zypper removerepo "amdgpu-proprietary"
sudo zypper removerepo "rocm"
sudo zypper removerepo "devel_languages_perl"

# Register repo for kernel module driver packages
sudo tee /etc/zypp/repos.d/amdgpu.repo <<EOF
[amdgpu]
name=amdgpu
baseurl=https://repo.radeon.com/amdgpu/6.0.2/sle/15.5/main/x86_64/
enabled=1
gpgcheck=1
gpgkey=https://repo.radeon.com/rocm/rocm.gpg.key
EOF

sudo zypper ref

# Register repo for ROCm packages
sudo tee --append /etc/zypp/repos.d/rocm.repo <<EOF
[ROCm-6.0.2]
name=ROCm6.0.2
baseurl=https://repo.radeon.com/rocm/zyp/6.0.2/main
enabled=1
gpgcheck=1
gpgkey=https://repo.radeon.com/rocm/rocm.gpg.key
EOF

sudo zypper ref

# The amdgpu-install version AMDGPU_ROCM_PACKAGE_URL amdgpu-install-6.0.60002 is version of ROCm
# The ROCm version must match the version in TORCH_ROCM_WHL_URL - rocm6.0
# The "cp310" in TORCH_ROCM_WHL_URL describes Python 3.10 
AMDGPU_ROCM_PACKAGE_URL=https://repo.radeon.com/amdgpu-install/6.0.2/sle/15.5/amdgpu-install-6.0.60002-1.noarch.rpm

# Install ROCm using amd install.
# Should work now when packages added.
# sudo zypper --no-gpg-checks install $AMDGPU_ROCM_PACKAGE_URL
# does not work : sudo amdgpu-install --usecase=graphics,rocm

# Install ROCM from native package manager
# sudo zypper addrepo https://download.opensuse.org/repositories/devel:languages:perl/SLE_15_SP5/devel:languages:perl.repo
sudo zypper addrepo https://download.opensuse.org/repositories/devel:/languages:/perl/openSUSE_Tumbleweed/

sudo zypper install kernel-default-devel
# See prerequisites. Adding current user to Video and Render groups
sudo usermod -a -G render,video $LOGNAME
sudo zypper --no-gpg-checks install $AMDGPU_ROCM_PACKAGE_URL
sudo zypper refresh
sudo zypper install amdgpu-dkms

echo "Please reboot system for all settings to take effect."
sudo zypper install rocm

echo "Please reboot system for all settings to take effect."

read

# Make sure to use 3.10
ML_PROJECT=$HOME/dev/my-projects-source/public-on-github/ai/transformers/llm/train-bert-on-yelp
cd $ML_PROJECT

# This is failing in Tumbleweed
# TORCH_ROCM_WHL_URL=https://repo.radeon.com/rocm/manylinux/rocm-rel-6.0.2/torch-2.1.2+rocm6.0-cp310-cp310-linux_x86_64.whl

# Install Torch from nightly wheel files.
# Alternative for TORCH_ROCM_WHL_URL is to specify nithgtly torch builds; they have WHL files for rocm6.0-cp310. Those seem to be automatically selected when we specify simply:
TORCH_ROCM_WHL_URL="--index-url https://download.pytorch.org/whl/nightly/rocm6.0"
cd HUGE-NO-BACKUP
pip3 install --force-reinstall --pre torch torchvision torchaudio $TORCH_ROCM_WHL_URL

# Next do post-torch-install steps
wget https://raw.githubusercontent.com/wiki/ROCmSoftwarePlatform/pytorch/files/install_kdb_files_for_pytorch_wheels.sh

##Optional; replace 'gfx90a' with your architecture and 5.6 with your preferred ROCm version
#export GFX_ARCH=gfx900
#
##Optional
#export ROCM_VERSION=6.0
#
## this does not work on suse it seems.
#./install_kdb_files_for_pytorch_wheels.sh

# FAILED - Try torch from docker

pip3 uninstall torch torchvision torchaudio

sudo zypper install docker

sudo systemctl enable docker.service
sudo systemctl start docker.service 
sudo docker pull rocm/pytorch:latest
sudo docker run -it --cap-add=SYS_PTRACE --security-opt seccomp=unconfined \
--device=/dev/kfd --device=/dev/dri --group-add video \
--ipc=host --shm-size 4G rocm/pytorch:latest

In the docker prompt, run:

HSA_OVERRIDE_GFX_VERSION=9.0.0 python3 -c "import torch; cuda0 = torch.device('cuda:0');print(torch.ones([2, 4], dtype=torch.float64, device=cuda0)); print('done')"

BUT IF FAILE WITH SAME ERROR: HSA_STATUS_ERROR_EXCEPTION: An HSAIL operation resulted in a hardware exception. code: 0x1016

============================


** After the above installation of ROCm (amdgpu) todo check if it worked and how


*** glxinfo | egrep -i 'device|memory'
    Device: llvmpipe (LLVM 15.0.7, 256 bits) (0xffffffff)
    Video memory: 14875MB
    Unified memory: yes
    GL_AMD_multi_draw_indirect, GL_AMD_pinned_memory, 
    GL_EXT_framebuffer_object, GL_EXT_framebuffer_sRGB, GL_EXT_memory_object, 
    GL_EXT_memory_object_fd, GL_EXT_packed_depth_stencil, GL_EXT_packed_float, 
    GL_AMD_pinned_memory, GL_AMD_query_buffer_object, 
    GL_EXT_gpu_program_parameters, GL_EXT_gpu_shader4, GL_EXT_memory_object, 
    GL_EXT_memory_object_fd, GL_EXT_multi_draw_arrays, 
    GL_EXT_map_buffer_range, GL_EXT_memory_object, GL_EXT_memory_object_fd, 

*** rocminfo | grep Name
  Name:                    AMD Ryzen 5 2500U with Radeon Vega Mobile Gfx
  Marketing Name:          AMD Ryzen 5 2500U with Radeon Vega Mobile Gfx
  Vendor Name:             CPU                                
  Name:                    gfx902                             
  Marketing Name:          AMD Radeon Graphics                
  Vendor Name:             AMD                                
  Name:                    amdgcn-amd-amdhsa--gfx902:xnack+   
     
*** rocminfo | grep gfx
  Name:                    gfx902                             
*** sudo dmesg | grep kfd
*** sudo dmesg | grep rocm
*** sudo modprobe -vv amdgpu # also try --vvr!
*** lsmod | grep amdgpu
amdgpu              13303808  42
amdxcp                 12288  1 amdgpu
i2c_algo_bit           20480  1 amdgpu
drm_ttm_helper         12288  1 amdgpu
ttm                   102400  2 amdgpu,drm_ttm_helper
drm_exec               16384  1 amdgpu
gpu_sched              65536  1 amdgpu
drm_suballoc_helper    12288  1 amdgpu
drm_buddy              20480  1 amdgpu
drm_display_helper    237568  1 amdgpu
video                  77824  2 acer_wmi,amdgpu
      Name:                    amdgcn-amd-amdhsa--gfx902:xnack+   

** FAILED INSTALL METHODS

============ NO LUCK - TRY REBUILD PYTORCH FROM GIT AS SUGGESTED HERE

https://github.com/ROCm/ROCm/issues/1743

Here is a workaround to run pytorch on gfx90c.
Just build pytorch for gfx900 and override gfx90c to gfx900.

# Build pytorch
git clone https://github.com/pytorch/pytorch.git  
cd pytorch  
git submodule update --init --recursive
pip3 install -r requirements.txt
pip3 install enum34 numpy pyyaml setuptools typing cffi future hypothesis typing_extensions
python3 tools/amd_build/build_amd.py
PYTORCH_ROCM_ARCH=gfx900 USE_ROCM=1 MAX_JOBS=4 python3 setup.py install

# Run an example
git clone https://github.com/pytorch/examples.git
cd examples/mnist
pip3 install -r requirements.txt
HSA_OVERRIDE_GFX_VERSION=9.0.0 python3 main.py

 the above failed *THIS BUILD OK BUT FAILED - HSA_STATUS_ERROR_MEMORY_APERTURE_VIOLATION*

AMD_LOG_LEVEL=5 HSA_OVERRIDE_GFX_VERSION=9.0.0 python


========================= FAILED - TRY TO DISABLE SOME THINGS IN HARDWARE GPU

According to https://github.com/ROCm/ROCm/issues/1743#issuecomment-1149902796
sudo modprobe amdgpu ppfeaturemask=0xfff73fff


======================== SET MORE UMA/VRAM MEMORY

sEE https://bbs.archlinux.org/viewtopic.php?id=283308


** LATEST INSTALL ATTEMPT AT TUMBLEWEED - REMOVE ALL PACKAGES, REINSTALL USING THE AMDGPU-INSTALL METHOD
# LATEST
# UP TO HERE CAN BE IGNORED. START HERE
sudo zypper remove amdgpu-dkms
sudo zypper remove amdgpu
sudo zypper remove rocm
sudo zypper remove rocm-*
sudo zypper rm amdgpu-core
sudo zypper rm amdgpu-*
# NO this removes Plasma and all: zypper rm kernel-firmware-amdgpu libdrm_amdgpu1
sudo zypper rm kernel-firmware-amdgpu

#
echo "Please reboot system for all settings to take effect."
#

sudo usermod -a -G render,video $LOGNAME

sudo zypper --no-gpg-checks install https://repo.radeon.com/amdgpu-install/6.0.2/sle/15.5/amdgpu-install-6.0.60002-1.noarch.rpm

# Something added additional ROCm repos, removing them manuall!!

# sudo amdgpu-install --usecase=graphics,rocm
sudo amdgpu-install --usecase=rocm # Ignoring 'No provider of amdgpu-dkms found'

*Installation has completed with error. BUT THINGS SEEM GENERALLY OK SO KEEP GOING*

#
echo "Please reboot system for all settings to take effect."
#

# Install Torch from nightly wheel files.
TORCH_ROCM_WHL_URL="--index-url https://download.pytorch.org/whl/nightly/rocm6.0"
cd HUGE-NO-BACKUP
pip3 install --force-reinstall --pre torch torchvision torchaudio $TORCH_ROCM_WHL_URL


* Install rocm and pytorch on Ubuntu

** Python before any changes

which python3 # /usr/bin/python3
python3 --version # Python 3.10.12

# nogo sudo apt install pyenv
# pyenv install 3.10

So I will not be using pyenv - will use python 3.10.12 native

** Install ROCm 6.0.2

sudo apt install "linux-headers-$(uname -r)" "linux-modules-extra-$(uname -r)"
# See prerequisites. Adding current user to Video and Render groups
sudo usermod -a -G render,video $LOGNAME
wget https://repo.radeon.com/amdgpu-install/6.0.2/ubuntu/jammy/amdgpu-install_6.0.60002-1_all.deb
sudo apt install ./amdgpu-install_6.0.60002-1_all.deb
sudo apt update
sudo apt install amdgpu-dkms
sudo apt install rocm
echo "Please reboot system for all settings to take effect."

** Install PyTorch using the wheel whl files from AMD for ROCm 6.0.2. - FAILED with HIP error: shared object initialization failed

wget https://repo.radeon.com/rocm/manylinux/rocm-rel-6.0.2/torch-2.1.2+rocm6.0-cp310-cp310-linux_x86_64.whl
wget https://repo.radeon.com/rocm/manylinux/rocm-rel-6.0.2/torchvision-0.16.1+rocm6.0-cp310-cp310-linux_x86_64.whl
pip3 install --force-reinstall torch-2.1.2+rocm6.0-cp310-cp310-linux_x86_64.whl torchvision-0.16.1+rocm6.0-cp310-cp310-linux_x86_64.whl

Successfully installed, with warning:

 WARNING: The script isympy is installed in '/home/mzimmermann/.local/bin' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
  WARNING: The script f2py is installed in '/home/mzimmermann/.local/bin' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
  WARNING: The script normalizer is installed in '/home/mzimmermann/.local/bin' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2 and torchrun are installed in '/home/mzimmermann/.local/bin' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.

*** Testing after installation

mzimmermann@acer-ryzen-laptop-wifi:~/dev/my-projects-source/public-on-github/ai/transformers/llm/train-bert-on-yelp$ glxinfo | egrep -i 'device|memory'
    Device: AMD Radeon Graphics (raven, LLVM 15.0.7, DRM 3.56, 6.5.0-25-generic) (0x15dd)
    Video memory: 1024MB
    Unified memory: no
Memory info (GL_ATI_meminfo):
    VBO free memory - total: 753 MB, largest block: 753 MB
    VBO free aux. memory - total: 7378 MB, largest block: 7378 MB
    Texture free memory - total: 753 MB, largest block: 753 MB
    Texture free aux. memory - total: 7378 MB, largest block: 7378 MB
    Renderbuffer free memory - total: 753 MB, largest block: 753 MB
    Renderbuffer free aux. memory - total: 7378 MB, largest block: 7378 MB
Memory info (GL_NVX_gpu_memory_info):
    Dedicated video memory: 1024 MB
    Total available memory: 8461 MB
    Currently available dedicated video memory: 753 MB
    GL_AMD_performance_monitor, GL_AMD_pinned_memory, 
    GL_EXT_framebuffer_object, GL_EXT_framebuffer_sRGB, GL_EXT_memory_object, 
    GL_EXT_memory_object_fd, GL_EXT_packed_depth_stencil, GL_EXT_packed_float, 
    GL_MESA_texture_signed_rgba, GL_NVX_gpu_memory_info, 
    GL_AMD_pinned_memory, GL_AMD_query_buffer_object, 
    GL_EXT_gpu_program_parameters, GL_EXT_gpu_shader4, GL_EXT_memory_object, 
    GL_EXT_memory_object_fd, GL_EXT_multi_draw_arrays, 
    GL_MESA_texture_signed_rgba, GL_MESA_window_pos, GL_NVX_gpu_memory_info, 
    GL_EXT_instanced_arrays, GL_EXT_map_buffer_range, GL_EXT_memory_object, 
    GL_EXT_memory_object_fd, GL_EXT_multi_draw_arrays, 
mzimmermann@acer-ryzen-laptop-wifi:~/dev/my-projects-source/public-on-github/ai/transformers/llm/train-bert-on-yelp$ rocminfo | grep Name
  Name:                    AMD Ryzen 5 2500U with Radeon Vega Mobile Gfx
  Marketing Name:          AMD Ryzen 5 2500U with Radeon Vega Mobile Gfx
  Vendor Name:             CPU                                
  Name:                    gfx902                             
  Marketing Name:          AMD Radeon Graphics                
  Vendor Name:             AMD                                
      Name:                    amdgcn-amd-amdhsa--gfx902:xnack+   
mzimmermann@acer-ryzen-laptop-wifi:~/dev/my-projects-source/public-on-github/ai/transformers/llm/train-bert-on-yelp$  dmesg | grep kfd
dmesg: read kernel buffer failed: Operation not permitted
mzimmermann@acer-ryzen-laptop-wifi:~/dev/my-projects-source/public-on-github/ai/transformers/llm/train-bert-on-yelp$ sudo dmesg | grep kfd
[    5.469797] kfd kfd: amdgpu: Allocated 3969056 bytes on gart
[    5.469828] kfd kfd: amdgpu: Total number of KFD nodes to be created: 1
[    5.470194] kfd kfd: amdgpu: added device 1002:15dd
mzimmermann@acer-ryzen-laptop-wifi:~/dev/my-projects-source/public-on-github/ai/transformers/llm/train-bert-on-yelp$ sudo dmesg | grep rocm
mzimmermann@acer-ryzen-laptop-wifi:~/dev/my-projects-source/public-on-github/ai/transformers/llm/train-bert-on-yelp$ sudo modprobe -vv amdgpu
modprobe: INFO: ../libkmod/libkmod.c:367 kmod_set_log_fn() custom logging function 0x648e67749830 registered
modprobe: INFO: ../libkmod/libkmod.c:334 kmod_unref() context 0x648e677dd460 released
mzimmermann@acer-ryzen-laptop-wifi:~/dev/my-projects-source/public-on-github/ai/transformers/llm/train-bert-on-yelp$ lsmod | grep amdgpu
amdgpu              16670720  25
amddrm_ttm_helper      12288  1 amdgpu
amdttm                110592  2 amdgpu,amddrm_ttm_helper
amddrm_buddy           20480  1 amdgpu
amdxcp                 12288  1 amdgpu
amd_sched              61440  1 amdgpu
amdkcl                 32768  3 amd_sched,amdttm,amdgpu
drm_display_helper    241664  1 amdgpu
drm_kms_helper        274432  4 drm_display_helper,amdgpu
drm                   765952  14 drm_kms_helper,amd_sched,amdttm,drm_display_helper,amdgpu,amddrm_buddy,amddrm_ttm_helper,amdxcp
i2c_algo_bit           16384  1 amdgpu
video                  73728  2 acer_wmi,amdgpu




mzimmermann@acer-ryzen-laptop-wifi:~/dev/my-projects-source/public-on-github/ai/transformers/llm/train-bert-on-yelp$ HSA_OVERRIDE_GFX_VERSION=9.0.0 python3 -c "import torch; cuda0 = torch.device('cuda:0');print(torch.ones([2, 4], dtype=torch.float64, device=cuda0)); print('done')"
Traceback (most recent call last):
  File "<string>", line 1, in <module>
RuntimeError: HIP error: shared object initialization failed
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing HIP_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.

mzimmermann@acer-ryzen-laptop-wifi:~/dev/my-projects-source/public-on-github/ai/transformers/llm/train-bert-on-yelp$ 

** Install PyTorch using NIGHTLY for ROCm 6.0.2 - Fails ERROR: HSA_STATUS_ERROR_MEMORY_APERTURE_VIOLATION: The agent attempted to access memory beyond the largest legal address.


TORCH_ROCM_WHL_URL="--index-url https://download.pytorch.org/whl/nightly/rocm6.0"
cd HUGE-NO-BACKUP
pip3 install --force-reinstall --pre torch torchvision  $TORCH_ROCM_WHL_URL


* Back to basics: Install ROCM 5.7, Torch 1.13 (latest Torch 1, latest ROCM 5


** Install PYENV

Pyenv is not available in the official Ubuntu package repository. However, you can install it manually12345. Here are the steps:

Install and Update Dependencies Start by updating system packages:
sudo apt update -y
Then, install all of Pyenv’s dependencies:

sudo apt install -y make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev python3-openssl git

Download the Script of Pyenv Download the script of Pyenv with the following command:

curl https://pyenv.run | bash

Set the Environment Up Run the following block of commands to set certain crucial environment variables and set up Pyenv autocompletion:

echo 'export PYENV_ROOT="$HOME/.pyenv"' >> ~/.bashrc
echo 'export PATH="$PYENV_ROOT/bin:$PATH"' >> ~/.bashrc
echo -e 'if command -v pyenv 1>/dev/null 2>&1; then\n eval "$(pyenv init -)"\nfi' >> ~/.bashrc
Finally, restart the shell to begin utilizing Pyenv:
exec "$SHELL"


** Install pyenv Python 3.9

pyenv install 3.9

** Uninstall ROCm 6.0.2

sudo apt autoremove rocm-core
# Or for version specific packages:
# sudo apt autoremove rocm-core6.0.2
sudo apt autoremove amdgpu-install
sudo apt autoremove amdgpu-dkms

# Remove the repositories.
sudo rm /etc/apt/sources.list.d/rocm.list
sudo rm /etc/apt/sources.list.d/amdgpu.list

# Clear the cache and clean the system.
sudo rm -rf /var/cache/apt/*
sudo apt-get clean all

# Restart the system.
echo "Check any errors, then hit any key to restart the system."
read

sudo reboot


** MY GENERIC env vars based ROCm + Pytorch installer on Ubuntu

# 1. Run ROCm uninstall above, then reboot


# 2. Download and convert the package signing key.
sudo mkdir --parents --mode=0755 /etc/apt/keyrings

# Download the key, convert the signing-key to a full
# keyring required by apt and store in the keyring directory
wget https://repo.radeon.com/rocm/rocm.gpg.key -O - | \
    gpg --dearmor | sudo tee /etc/apt/keyrings/rocm.gpg > /dev/null

# 3. Set variables defining the ROCm package and URL names		
#ROCM_VERSION_6060002=5.7.50703-1
#ROCM_VERSION_602=5.7.3
#ROCM_VERSION_60=5.7

ROCM_VERSION_6060002=6.0.60002-1
ROCM_VERSION_602=6.0.2
ROCM_VERSION_60=6.0

# 4. Add repositories

# amdgpu repo for jammy - should use latest, not ${ROCM_VERSION_602}
# see https://github.com/nktice/AMD-AI - NO NO LUCK
echo "deb [arch=amd64 signed-by=/etc/apt/keyrings/rocm.gpg] https://repo.radeon.com/amdgpu/${ROCM_VERSION_602}/ubuntu jammy main" \
    | sudo tee /etc/apt/sources.list.d/amdgpu.list
sudo apt update -y 

# ROCm repository
echo "deb [arch=amd64 signed-by=/etc/apt/keyrings/rocm.gpg] https://repo.radeon.com/rocm/apt/${ROCM_VERSION_602} jammy main" \
    | sudo tee --append /etc/apt/sources.list.d/rocm.list
echo -e 'Package: *\nPin: release o=repo.radeon.com\nPin-Priority: 600' \
    | sudo tee /etc/apt/preferences.d/rocm-pin-600
sudo apt update -y


# Check enabled repositories
sudo grep -rhE ^deb /etc/apt/sources.list*

# 5. Download and install AMDGPU packages
sudo apt install "linux-headers-$(uname -r)" "linux-modules-extra-$(uname -r)"
# See prerequisites. Adding current user to Video and Render groups
sudo usermod -a -G render,video $LOGNAME

AMDGPU_ROCM_PACKAGE_FILE=amdgpu-install_${ROCM_VERSION_6060002}_all.deb
AMDGPU_ROCM_PACKAGE_URL=https://repo.radeon.com/amdgpu-install/${ROCM_VERSION_602}/ubuntu/jammy/${AMDGPU_ROCM_PACKAGE_FILE}
wget ${AMDGPU_ROCM_PACKAGE_URL}

#wget https://repo.radeon.com/amdgpu-install/6.0.2/ubuntu/jammy/amdgpu-install_6.0.60002-1_all.deb
#wget https://repo.radeon.com/amdgpu-install/5.7.3/ubuntu/jammy/amdgpu-install_5.7.50703-1_all.deb


# Install ROCm using amd install.
# Should work now when packages added.
# sudo zypper --no-gpg-checks install $AMDGPU_ROCM_PACKAGE_URL
 sudo apt install ./$AMDGPU_ROCM_PACKAGE_FILE
#sudo apt install ./amdgpu-install_5.7.50703-1_all.deb
sudo apt update

# 6. Install amdgpu-dkms and rocm
sudo apt install amdgpu-dkms
sudo apt install rocm

# Check enabled repositories again
sudo grep -rhE ^deb /etc/apt/sources.list*

echo "Please reboot system for all settings to take effect."

** Install PyTorch using the wheel whl files from AMD for ROCm 5.7.3

PYTHON_VERSION_310=310 # COULD BE 39 or 310
TORCH_VERSION=2.1.2 # COULD BE 2.1.2 or 1.13.1
TORCHVISION_VERSION=0.16.1 # COULD BE 0.16.1 or 0.14.1

#TORCH_ROCM_WHL_URL=https://repo.radeon.com/rocm/manylinux/rocm-rel-6.0.2/torch-2.1.2+rocm6.0-cp310-cp310-linux_x86_64.whl
#TORCH_ROCM_WHL_URL=https://repo.radeon.com/rocm/manylinux/rocm-rel-5.7/torch-1.13.1+rocm5.7-cp39-cp39-linux_x86_64.whl

#TORCH_ROCM_WHL_FILE=torch-1.13.1+rocm${ROCM_VERSION_60}-cp${PYTHON_VERSION_310}-cp${PYTHON_VERSION_310}-linux_x86_64.whl
#TORCH_ROCM_WHL_URL=https://repo.radeon.com/rocm/manylinux/rocm-rel-${ROCM_VERSION_60}/$TORCH_ROCM_WHL_FILE
#TORCHVISION_ROCM_WHL_FILE=torchvision-0.14.1+rocm${ROCM_VERSION_60}-cp${PYTHON_VERSION_310}-cp${PYTHON_VERSION_310}-linux_x86_64.whl
#TORCHVISION_ROCM_WHL_URL=https://repo.radeon.com/rocm/manylinux/rocm-rel-${ROCM_VERSION_60}/$TORCHVISION_ROCM_WHL_FILE

 TORCH_ROCM_WHL_FILE=torch-${TORCH_VERSION}+rocm${ROCM_VERSION_60}-cp${PYTHON_VERSION_310}-cp${PYTHON_VERSION_310}-linux_x86_64.whl
 TORCH_ROCM_WHL_URL=https://repo.radeon.com/rocm/manylinux/rocm-rel-${ROCM_VERSION_602}/$TORCH_ROCM_WHL_FILE
 TORCHVISION_ROCM_WHL_FILE=torchvision-${TORCHVISION_VERSION}+rocm${ROCM_VERSION_60}-cp${PYTHON_VERSION_310}-cp${PYTHON_VERSION_310}-linux_x86_64.whl
 TORCHVISION_ROCM_WHL_URL=https://repo.radeon.com/rocm/manylinux/rocm-rel-${ROCM_VERSION_602}/$TORCHVISION_ROCM_WHL_FILE

wget $TORCH_ROCM_WHL_URL
wget $TORCHVISION_ROCM_WHL_URL
pip3 install --force-reinstall $TORCH_ROCM_WHL_FILE $TORCHVISION_ROCM_WHL_FILE


# Note: Possible missing firmware /lib/firmware/amdgpu/vega10_cap.bin BUT NO MENTION OF VEGA8!! MAYBE THAT MEANS THE PACKAGE WAS NOT BUILD FOR IT? See https://git.kernel.org/pub/scm/linux/kernel/git/firmware/linux-firmware.git/tree/amdgpu for firmware location




* Interesting comments

From https://en.opensuse.org/SDB:AMD_GPGPU
GCN5.0: The last ROCm with support for GCN 5.0 (Vega 10, Raven, Picasso, gfx900) is 4.5.2. This is the iGPU on 2500U So try ROCm 4.5.2? - no, 4.5.2 is not even on the rocm website!!



* Another try: Build torch and torchvision locally!!!!

See https://github.com/ROCm/ROCm/issues/2689

terryr@theblob:~$ cat ./local_build.sh 

source /mnt/data/automatic1111/stable-diffusion-webui/venv/bin/activate

export PYTORCH_ROCM_ARCH=gfx1100

export USE_NINJA=1
export USE_CUDA=0 
export USE_ROCM=1 
export USE_LMDB=1 
export USE_OPENCV=1 
export MAX_JOBS=10


cd pytorch
git pull --recurse-submodules
git pull

python tools/amd_build/build_amd.py
python setup.py build install

./.ci/pytorch/build.sh


cd ..


cd vision
git pull --recurse-submodules 

python setup.py develop

python setup.py build install

cd ..


cd audio

git pull --recurse-submodules 
python setup.py develop

python setup.py build install

cd ..
 
