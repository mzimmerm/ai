* TL;DR for this project, once everything is installed

jupyter-lab train-bert-on-yelp-local.ipynb : Start notebook


* Running Huggingface train for Bert with Yelp dataset

- Go to https://huggingface.co/docs/transformers/training

- Click "Open in Colab", select "mixed". Colab 

- This opens the whole article in Google Colab. Each piece of code is a cell.

- The cell sections are as follows:

  - Top: 'pip install transformers datasets'
    - Only this line runs
  - Fine-tune a pretrained model
    1. Prepare a dataset
    2. Train (no code)
    3. Train with PyTorch Trainer
    4. Train a TensorFlow model with Keras
    5. Train in native PyTorch

  - We will ONLY run 1,3: 'Prepare a dataset', and 'Train with Pytorch Trainer'

    1. Prepare a dataset
       - The 'tokenize' section takes about 10 minutes
         #+begin_src python
           tokenized_datasets = dataset.map(tokenize_function, batched=True)
         #+end_src

       - The 'tokenized_datasets' instance is DatasetDict, contains 2 Dataset instances, probably large
       - Small datasets:
         #+begin_src python
           small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
           small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
           print(small_eval_dataset)
           print(small_eval_dataset.data[0][1:3]) # label (0-5)
           print(small_eval_dataset.data[1][1:3]) # text of review
         #+end_src
         - Result:
           #+begin_example
             small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
             small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
             print(small_eval_dataset)
             print(small_eval_dataset.data[0][1:3]) # label (0-5)
             print(small_eval_dataset.data[1][1:3]) # text of review

           #+end_example
         - The 'small_train_dataset' and 'small_eval_dataset' instances are of type Dataset

    2. Train with PyTorch Trainer
       - Import Model class and create model, defining only the features from above.
         #+begin_src python
           from transformers import AutoModelForSequenceClassification

           model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5) # labels correspond to 5 Dataset features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],
         #+end_src
       - The 'model' instance has tensors, about 400M! This must be the Bert model already trained, that is why so large
       - Training hyperparameters
         - This section uses default 'hyperparameters'


* Running Locally

** For local runnning, I use python virtual environment ~/software/python/venv3.11~

- cd this directory
- source \~/software/python/venv3.11/bin/activate
- 

           

* Useful linux and python commands for AI

** Synchronize

On server, synchronize the 'ai' directory to laptop
# rsync the "ai" directory from server to laptop
# remove the --dry-run

rsync --dry-run --verbose --mkpath --archive /home/mzimmermann/dev/my-projects-source/public-on-github/ai mzimmermann@acer-ryzen-laptop-wifi:~/dev/my-projects-source/public-on-github


** Find and change VRAM used by GPU

# Find memory (vram) aveilable to AMG GPU
lspci -D # Find line that looks like graphics, e.g. "0000:03:00.0 VGA
compatible controller: ..."
# Then run 
cat /sys/bus/pci/devices/0000:03:00.0/mem_info_vram_total # Shows total VRAM size

# Another way
glxinfo | egrep -i 'device|memory'

AFTER rocm install:
localhost:~ # glxinfo | egrep -i 'device|memory'
    Device: llvmpipe (LLVM 15.0.7, 256 bits) (0xffffffff)
    Video memory: 14875MB
    Unified memory: yes
    GL_AMD_multi_draw_indirect, GL_AMD_pinned_memory, 
    GL_EXT_framebuffer_object, GL_EXT_framebuffer_sRGB, GL_EXT_memory_object, 
    GL_EXT_memory_object_fd, GL_EXT_packed_depth_stencil, GL_EXT_packed_float, 
    GL_AMD_pinned_memory, GL_AMD_query_buffer_object, 
    GL_EXT_gpu_program_parameters, GL_EXT_gpu_shader4, GL_EXT_memory_object, 
    GL_EXT_memory_object_fd, GL_EXT_multi_draw_arrays, 
    GL_EXT_map_buffer_range, GL_EXT_memory_object, GL_EXT_memory_object_fd, 

See this topic:

https://bbs.archlinux.org/viewtopic.php?id=283308

which explains to use AMD tool to change VRAM in BIOS. BUT with ROCm
installed, it appears all memory is available for video
  
  



* todo Addendums to ROCm install

** todo CUDA CUDA, which stands for Compute Unified Device Architecture, is a parallel computing platform and programming model developed by NVIDIA

* Install ROCm on Linux, for AMD Ryzen APUs

I have AMD APU Ryzen 5 2500U.

This text is only concerned with Linux running AMD software for iGPU (GPU
integrated in APU).

AMD was late in software support for their APUs and GPUs to run machine
learning (ML) training. Simplifying, we can say that the Python PyTorch
package is the standard way to run ML algorithms. While PyTorch can run on
both CPU and GPU, only GPU processing is reasonably performant. GPU processing
needs support from low level software on the GPU. AMD's GPU low level software
is ROCm. todo: how does AMD describe ROCm? ROCm, officially supports a 
miniscule number of discrete video cards; it does not officially support any
iGPU, see
https://rocm.docs.amd.com/projects/radeon/en/latest/docs/compatibility.html. 
A bug report to support ROCm on 4-year old Renoir GPUs is closed with
(paraphrasing) "not supported, some have success running it", see
https://github.com/ROCm/ROCm/issues/1101. Not exactly a vote of confidence.


** Installing ROCm on Linux

Presumably (obviously) one does have to install ROCm software and drivers on their system to run ML
efficiently. AMD's documentation of is confusing, it is not clear where to start, we have

1. https://rocm.docs.amd.com/projects/radeon/en/latest/index.html
2. https://rocm.docs.amd.com/en/latest/
3. https://www.amd.com/en/support/linux-drivers
4. https://community.amd.com/t5/ai/amd-extends-support-for-pytorch-machine-learning-development-on/ba-p/637756
5. todo: add lmstudio links, explain differences.


Look-ahead note: After looking into the instructions, there are two distinct
elements: Something called "amdgpu" and "rocm". It appears that AMD uses the
term "amdgpu" when it refers to the whole package. "rocm" seems to be the name
of the kernel driver, so it is a "part of" the "amdgpu" package.

Deeper in the second link, we find 
https://rocm.docs.amd.com/projects/install-on-linux/en/latest/tutorial/quick-start.html
which offers two methods of installing ROCm on Linux.

- Native package manager
- AMDGPU installer

Native package manager is a Linux-distro specific package, offered for Debian,
RedHat, and SuSE's SLE.

AMDGPU installer is also a Linux-distro specific package, offered for the same
distros.

The installer needs to be executed, the native package installation requires a
few more manual steps to install the driver.  In line with the AMD's ROCm
confusion, it is not clear why there are two methods, which is better or
preferred and what are the differences between them.

In my installation, I am using the "Native package manager" approach on SuSE Leap 15.5. I know, Leap !=
SLE, but they should be interchangeable, and the process worked, as you can
see if you keep reading. These are the installation steps from https://rocm.docs.amd.com/projects/install-on-linux/en/latest/tutorial/quick-start.html#suse-linux-enterprise-server:

----
sudo zypper addrepo https://download.opensuse.org/repositories/devel:languages:perl/SLE_15_SP5/devel:languages:perl.repo
sudo zypper install kernel-default-devel
# See prerequisites. Adding current user to Video and Render groups
sudo usermod -a -G render,video $LOGNAME
sudo zypper --no-gpg-checks install https://repo.radeon.com/amdgpu-install/6.0.2/sle/15.5/amdgpu-install-6.0.60002-1.noarch.rpm
sudo zypper refresh
sudo zypper install amdgpu-dkms
sudo zypper install rocm
echo "Please reboot system for all settings to take effect."
----

The above steps ran with no error on OpenSUSe Leap 15.5.

Rebooted the system.

todo improve: Added latest-2,latest-3 to /etc/zypp/zypp.conf


** Validating ROCm installation

After the above installation of ROCm (amdgpu) todo check if it worked and how


AFTER rocm install:
localhost:~ # glxinfo | egrep -i 'device|memory'
    Device: llvmpipe (LLVM 15.0.7, 256 bits) (0xffffffff)
    Video memory: 14875MB
    Unified memory: yes
    GL_AMD_multi_draw_indirect, GL_AMD_pinned_memory, 
    GL_EXT_framebuffer_object, GL_EXT_framebuffer_sRGB, GL_EXT_memory_object, 
    GL_EXT_memory_object_fd, GL_EXT_packed_depth_stencil, GL_EXT_packed_float, 
    GL_AMD_pinned_memory, GL_AMD_query_buffer_object, 
    GL_EXT_gpu_program_parameters, GL_EXT_gpu_shader4, GL_EXT_memory_object, 
    GL_EXT_memory_object_fd, GL_EXT_multi_draw_arrays, 
    GL_EXT_map_buffer_range, GL_EXT_memory_object, GL_EXT_memory_object_fd, 

it appears all memory is available for video

BUT
> rocminfo
ROCk module is NOT loaded, possibly no GPU devices

So it appears rocm was not installed?? todo what does it mean??

#> dmesg | grep kfd
no output

#> dmesg | grep rocm
no output

So I did:

zypper in amdgpu # this is not in instructions todo- explain.

But it did not help

mv /etc/modprobe.d/blacklist-radeon.conf ~/tmp

modprobe amdgpu
modprobe: ERROR: could not insert 'amdgpu': Key was rejected by service

todo: this seems important:

localhost:/etc/modprobe.d # modprobe rocm
modprobe: FATAL: Module rocm not found in directory /lib/modules/5.14.21-150500.55.52-default

#> sudo modinfo amdgpu
shows some massive amount of stuff. maybe this is a path??

als

#> sudo modprobe -vvr amdgpu

#> mzimmermann@localhost:~/tmp> sudo modprobe -vv amdgpu
modprobe: INFO: custom logging function 0x55a45ae0d260 registered
insmod /lib/modules/5.14.21-150500.55.52-default/updates/amdkcl.ko.zst 
modprobe: INFO: Failed to insert module '/lib/modules/5.14.21-150500.55.52-default/updates/amdkcl.ko.zst': Key was rejected by service
modprobe: ERROR: could not insert 'amdgpu': Key was rejected by service
modprobe: INFO: context 0x55a45b3ad440 released


so going after amdkcl.ko
this has someting to do with secure boot - should be disabled

sudo mokutil --sb-state
SecureBoot enabled

So try to disable it???

From https://github.com/linux-surface/linux-surface/issues/906 it looks like
installing ssl could install a secure boot key, which fixes the problem??

According to the DKMS github page, on most distro, the key pair is located at /var/lib/dkms/[mok.key & mok.pub]. If the file is not present, it will generate one using openssl package.

On Ubuntu though, it will be configured to use the Ubuntu master key, which is
located in: /var/lib/shim-signed/mok/MOK.der and
/var/lib/shim-signed/mok/MOK.priv which is generated and enrolled when you
first install Ubuntu, where it will automatically configure SecureBoot.

So I did:

Yast->bootloader, uncheck "secure boot support"
reboot
THE ABOVE ALMOST FIXED IT, BUT SYSTEM DID NOT BOOT. I HAD TO ADD A PASSWORD TO
BIOS, THEN DISABLE SECURE BOOT IN BIOS.

AFTER REBOOT, rocm and amdgpu loaded:

localhost:~ # rocminfo | grep gfx
  Name:                    gfx902                             
      Name:                    amdgcn-amd-amdhsa--gfx902:xnack+   


localhost:~ # modprobe -vv amdgpu
modprobe: INFO: custom logging function 0x5604f440d260 registered
modprobe: INFO: context 0x5604f5db1690 released


Running a script that tests everything rocm. The script is mentioned in
https://github.com/ROCm/ROCm/issues/2216 and is here:
https://gist.github.com/damico/484f7b0a148a0c5f707054cf9c0a0533

Save the script as, for example  misc/test-rocm.py and run


- source ~/software/python/venv3.6-for-ai-rocm/bin/activate
- python misc/test-rocm.py

  (venv3.6-for-ai-rocm) mzimmermann@localhost:~/dev/my-projects-source/public-on-github/ai/transformers/llm/train-bert-on-yelp> python misc/test-rocm.py


Checking ROCM support...
GOOD: ROCM devices found:  2
Checking PyTorch...
GOOD: PyTorch is working fine.
Checking user groups...
GOOD: The user mzimmermann is in RENDER and VIDEO groups.
BAD: PyTorch ROCM support NOT found.

  
So ROCM is found, PyTorch works, but it does not have ROCm support.

That means , we have to install PyTorch that works with ROCm. This is done in
the next step

** Installing PyTorch that works with ROCm

Pytorch is a Python package for ML. As a reasonably complex Python package, it
will only work on certain Python versions. The default Python
of SLE 15 and OpenSuse Leap 15.5 is Python 3.6. So we have to install Pytorch
that works with Python 3.6. There is a site which allows to
select operating system, language, package, and platform (CUDA or ROCm), and
download PyTorch for the selected version. The site is
https://pytorch.org/get-started/locally/.

file:./misc/pytorch-installation.png

Problem is, it says it only works on Python 3.8 or higher - while SLE 15 and OpenSuse Leap 15.5 only support Python
3.6. How do we solve this??? todo

If we are not in venv, run this:

- source ~/software/python/venv3.6-for-ai-rocm/bin/activate
- then the command listed
- pip3 install --pre torch torchvision torchaudio --index-url
  https://download.pytorch.org/whl/nightly/rocm6.0
  
We get:

Looking in indexes: https://download.pytorch.org/whl/nightly/rocm6.0
Requirement already satisfied: torch in /home/mzimmermann/software/python/venv3.6-for-ai-rocm/lib/python3.6/site-packages (1.10.2)
ERROR: Could not find a version that satisfies the requirement torchvision (from versions: none)
ERROR: No matching distribution found for torchvision


For now, let's say we do NOT care about torchvision and torchaudio, we can
just install torch:

-  pip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/rocm6.0
(requirement already satisfied in my case)

So run the scripot again:
(venv3.6-for-ai-rocm) mzimmermann@localhost:~/dev/my-projects-source/public-on-github/ai/transformers/llm/train-bert-on-yelp> python misc/test-rocm.py


Checking ROCM support...
GOOD: ROCM devices found:  2
Checking PyTorch...
GOOD: PyTorch is working fine.
Checking user groups...
GOOD: The user mzimmermann is in RENDER and VIDEO groups.
BAD: PyTorch ROCM support NOT found.
(venv3.6-for-ai-rocm)
mzimmermann@localhost:~/dev/my-projects-source/public-on-github/ai/tran

SAME ERROR, WHY??

I will try to follow this solution:

https://github.com/pytorch/pytorch/issues/120433

RUN THIS:

- pip3 uninstall torch
- pip3 install --pre torch --index-url https://repo.radeon.com/rocm/manylinux/rocm-rel-6.0.2/torch-2.1.2+rocm6.0-cp310-cp310-linux_x86_64.whl

  ERROR

  Looking in indexes: https://repo.radeon.com/rocm/manylinux/rocm-rel-6.0.2/torch-2.1.2+rocm6.0-cp310-cp310-linux_x86_64.whl
ERROR: Could not find a version that satisfies the requirement torch (from versions: none)
ERROR: No matching distribution found for torch

SAME ERROR ON SERVER - I UNINSTALLED AND F***ED TORCH THERE AS WELL...

HOW TO FIX??

Trying this from https://rocm.docs.amd.com/_/downloads/radeon/en/latest/pdf/

- pip3 install --upgrade pip wheel

NOTE: wheel, todo what is it??

1) wget
   https://repo.radeon.com/rocm/manylinux/rocm-rel-6.0.2/torchvision-0.16.1+rocm6.0-cp310-cp310-linux_x86_64.whl
2) wget https://repo.radeon.com/rocm/manylinux/rocm-rel-6.0.2/torch-2.1.2+rocm6.0-cp310-cp310-linux_x86_64.whl
3) pip3 install --force-reinstall torch-2.1.2+rocm6.0-cp310-cp310-linux_x86_64.whl
 ERROR: torch-2.1.2+rocm6.0-cp310-cp310-linux_x86_64.whl is not a supported wheel on this platform.

 The 'not supported wheel on this platform', refers to the fact the WHL is
   cp310 for python 3.10 but the environment is 3.6

   I NEED TO INSTALL PYENV ON LEAP, THEN INSTALL 3.10 INTO IT. SEE
   https://unix.stackexchange.com/questions/757039/install-additional-python-on-opensuse-without-breaking-existing-pythons


*** Software packages which allow to use any Python version

The above problem - AMD providing a Python package that requires a specific Python version (3.10, indicated by the *cp310* in the name) - is very common. We need Python 3.10, but is is not readily available on our system (Leap 15.5, but we would hit the same issure in Debian, RedHat etc).

There are in 3 Python related pieces of software which let us to work around the problem. See for example https://betterstack.com/community/questions/what-are-differences-between-python-virtual-environments/. All of them allow to create virtual environments that allow to either install and/or switch between Python versions. But each of the 3 has their pros and cons:

1. venv is Python build-in module
   - Pros: Build in Python, relatively simple to setup a virtual environment for multiple Python versions
   - Cons: It can only use and switch between Python versions available on the OS. If we need some really old, or really new Python version for which the OS doeas not have a system level package, we are out of luck. This is the case for us: There is no Python3.10 installable on Opensuse Leap 15.5, so we cannot use venv
2. pyvenv is Python script on top of venv.
   - Pros: As above
   - Cons: As above
3. pyenv is a OS-level thing. 
   - Pros:
     - It can install a massive variety of multiple Python versions, literally hundreds, pretty much any Python version and flavour that ever existed
     - The multiple versions are installed in userspace (user home directory)
     - User can switch between the versions globally (per user), locally (per directory), or for shell (per current shell lifetime)
   - Cons:
     - It must build the needed version from source. That means, various "devel" packages must be installed on our OS. Which packages? That depends on the OS. For Opensuse, RedHat, and Debian, see for example https://realpython.com/intro-to-pyenv/#build-dependencies
     - Corollary of the above, the Python version must be buildable on the OS. 

       
*** Using *pyenv* to install the Python version for which AMD provides PyTorch on ROCm (3.10 at the time of writing, March 2024)

Because ther is no official Python3.10 package for our OS (Opensuse leap 15.5), our only choice is using *pyenv*.

Let us list the status of Python on my system before installation:

#+BEGIN_SRC bash :results raw
zypper search --installed-only python | grep -v "-"
#+END_SRC

| Name             | Summary                                   |
| libpython3_6m1_0 | Python Interpreter shared library package |
| python3          | Python 3 Interpreter package              |

There is only python3, python3.6, python3.6m, all linked to python3.6

#+BEGIN_SRC bash :results raw
python3 --version
#+END_SRC

#+RESULTS:
Python 3.6.15

Now let us go ahead and install pyenv

#+BEGIN_SRC bash :results raw
sudo zypper install pyenv
#+END_SRC

For curiosity, list the versions pyenv can install for us. The list is massive, we just shoe a subset:

#+BEGIN_SRC bash
pyenv install --list
#+END_SRC

#+RESULTS:
|       Available |
|-----------------|
|          3.10.0 |
|        3.10-dev |
|          3.10.1 |
|          3.10.7 |
|       3.11.0rc2 |
|  anaconda-1.4.0 |
| stackless-3.7.5 |

The list in the result of 'pyenv install --list' comes from official Python releases and other sources.

Now we need to select the version we want to use for Pytorch-on-ROCm. In that, we are limited to the version(s) for which AMD builds and tests their Pytorch-on-ROCm. These versions are available in todo

AMD provides us with PyTorch build for Python 3.10. Let's use pyenv to install Python3.10.7. First we need to set some pyenv-used environment variables:

#+BEGIN_SRC sh
  bashrcFilename=~/.bash_profile  # Avoid potential bash issueof loop in eval. Prefer this if using bash.
  # Alternatives of the above
  # bashrcFilename=~/.bashrc      # For non-bash users
  # bashrcFilename=/etc/profile.local # needs sudo
  # bashrcFilename=/etc/bash.bashrc.local # needs sudo

  echo 'export PYENV_ROOT="$HOME/.pyenv"' >> $bashrcFilename
  echo 'command -v pyenv >/dev/null || export PATH="$PYENV_ROOT/bin:$PATH"' >> $bashrcFilename
  echo 'eval "$(pyenv init -)"' >> $bashrcFilename
#+END_SRC

*Logout and log back in to enforce running the profile file during login shell start*, as we changed profile above. (We only need to start a new konsole if we changed bashrc)

Note: The ~pyenv init -~ which runs at the login shell, or startup of a new shell, establishes the directory ~$HOME/.pyenv~ with pyenv shims and versions. So after logging out and in, $PYENV_ROOT is set to ~$HOME/.pyenv~. 

Having initialized pyenv, we are ready to start installing Python versions that may not be part of the system. They will be compiled and installed to the  ~$HOME/.pyenv/versions directory. We will install Python3.10.7 as follows:

#+BEGIN_SRC sh :results raw
  pyenv install 3.10.7
#+END_SRC

There are errors and warnings from the above install, such as
#+BEGIN_EXAMPLE
  ERROR: The Python ssl extension was not compiled. Missing the OpenSSL lib?
#+END_EXAMPLE

The reason is, the system is missing some development packages. This is where we pay the price for pyenv ability to install any Python version - we have to "know" what system packages to install for the the ~pyenv install~ to work, and add those packages. This will differ from system to system. On Opensuse Leap 15.5, we need to install the following:

#+BEGIN_SRC bash
sudo zypper in zlib-devel bzip2 libbz2-devel libffi-devel libopenssl-devel readline-devel sqlite3 sqlite3-devel xz xz-devel
#+END_SRC

Now run the install again

#+BEGIN_SRC sh :results raw
  pyenv install 3.10.7
#+END_SRC

it should succeed, and install Python to ~$HOME/.pyenv/versions/3.10.7/~.

Now, when we run
#+BEGIN_SRC sh :results raw
  which python3
  python3 --version
#+END_SRC

#+RESULTS:
/home/mzimmermann/.pyenv/shims/python3
Python 3.6.15

We can see that python3 is still the system version, which is good.

Having the desired version installed, pyenv allows 3 ways of using the installed version3.10.7: "shell", "local", "global" - see https://github.com/pyenv/pyenv?tab=readme-ov-file#switch-between-python-versions.

In brief, to setup our command line to use the version, we can use one of the three commands.
#+BEGIN_SRC sh
pyenv shell 3.10.7  # select just for current shell session
pyenv local 3.10.7  # select when you are in the current directory (or subdirectories)
pyenv global 3.10.7 #  select globally for your user account
#+END_SRC

In our use of a ML project that uses PyTorch on ROCm, we will want to create a directory, say ~$ML_PROJECT~ in which our code will run using Python3.10.7. The following is a one-time command we need to run for any commands from ~$ML_PROJECT~ use Python3.10.7 in the future:

#+BEGIN_SRC sh :results raw
  ML_PROJECT=$HOME/dev/my-projects-source/public-on-github/ai/transformers/llm/train-bert-on-yelp
  cd $ML_PROJECT
  pyenv local 3.10.7
#+END_SRC

To confirm that is indeed true, try this
#+BEGIN_SRC sh :results raw
  ML_PROJECT=$HOME/dev/my-projects-source/public-on-github/ai/transformers/llm/train-bert-on-yelp
  echo When in the directory for which we ran 'pyenv local 3.10.7', the 3.10.7 is used
  cd $ML_PROJECT
  which python3     # From $HOME/.pyenv/shims
  which python      # As above
  which pip3        # As above
  which pip         # As above
  python3 --version # 3.10.7, the pyenv version
  python --version  # as above
  pip3 --version     # 22.2.2, the pyenv version
  pip --version
  echo When in the directory above, the OS version is used
  cd ..
  which python3     # From $HOME/.pyenv/shims, but directed to OS version, see below
  python3 --version # 3.6.15, the OS version
  python --version  # no python
  which pip3        # From $HOME/.pyenv/shims, but directed to OS version, see below
  pip3 --version    # 20.0.2, the OS version
#+END_SRC

#+RESULTS:
When in the directory for which we ran pyenv local 3.10.7, the 3.10.7 is used
/home/mzimmermann/.pyenv/shims/python3
/home/mzimmermann/.pyenv/shims/python
/home/mzimmermann/.pyenv/shims/pip3
/home/mzimmermann/.pyenv/shims/pip
Python 3.10.7
Python 3.10.7
pip 22.2.2 from /home/mzimmermann/.pyenv/versions/3.10.7/lib/python3.10/site-packages/pip (python 3.10)
pip 22.2.2 from /home/mzimmermann/.pyenv/versions/3.10.7/lib/python3.10/site-packages/pip (python 3.10)
When in the directory above, the OS version is used
/home/mzimmermann/.pyenv/shims/python3
Python 3.6.15
/home/mzimmermann/.pyenv/shims/pip3
pip 20.0.2 from /usr/lib/python3.6/site-packages/pip (python 3.6)

The functionality of using the intended Python version when in the directory ~$ML_PROJECT~ is achieved by the presence of the file ~.python-version~ in the directory, so do not delete the file.



*** Install AMD's PyTorch-on-ROCm

Now we are ready to install AMD's PyTorch-on-ROCm, using the AMD provided builds, as described in https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/install-pytorch.html, with additional motivation of WHY to use only the AMD builds of PyTorch discussed in https://github.com/pytorch/pytorch/issues/120433. 


RUN
#+BEGIN_SRC sh :results raw

  # This directory was setup to use pyenv Python3.10.7
  ML_PROJECT=$HOME/dev/my-projects-source/public-on-github/ai/transformers/llm/train-bert-on-yelp
  cd $ML_PROJECT

  # In case non-AMD versions are installed. Probably not needed with force-reinstall
  pip3 uninstall torch torchvision

  # Download AMD's PyTorch-on-ROCm
  wget https://repo.radeon.com/rocm/manylinux/rocm-rel-6.0.2/torch-2.1.2+rocm6.0-cp310-cp310-linux_x86_64.whl
  wget https://repo.radeon.com/rocm/manylinux/rocm-rel-6.0.2/torchvision-0.16.1+rocm6.0-cp310-cp310-linux_x86_64.whl

  # Force reinstall torch and torchvision
  PYTORCH_ROCM_ARCH=gfx900 USE_ROCM=1 MAX_JOBS=4 pip3 install --force-reinstall torch-2.1.2+rocm6.0-cp310-cp310-linux_x86_64.whl torchvision-0.16.1+rocm6.0-cp310-cp310-linux_x86_64.whl

#+END_SRC

The ~pip3 install~ installed torch, torchvision, and many packages they depend on to the pyenv directory ~$HOME/.pyenv/versions/3.10.7/lib/python3.10/site-packages~.

Now we can test if the installed PyTorch (refered above as PyTorch-on-ROMm) actually uses the GPU. If it does, a simple Python code should respond "True" to CUDA being available. NOTE THAT CUDA DOES NOT REFER TO NVIDIA, IT MERELY STATES THAT A LOW LEVEL GPU LIBRARY (ROCm IN OUR CASE) IS AVAILABLE TO PYTORCH.

#+BEGIN_SRC sh

  # This directory was setup to use pyenv Python3.10.7
  ML_PROJECT=$HOME/dev/my-projects-source/public-on-github/ai/transformers/llm/train-bert-on-yelp
  cd $ML_PROJECT
  
  python << EOF
  import torch
  print(torch.cuda.is_available())
  EOF
#+END_SRC

#+RESULTS:
: True

For completeness, run a script that tests both rocm and pytorch installation and running. The script is mentioned in https://github.com/ROCm/ROCm/issues/2216 and is here: https://gist.github.com/damico/484f7b0a148a0c5f707054cf9c0a0533

#+BEGIN_SRC sh

  # This directory was setup to use pyenv Python3.10.7
  ML_PROJECT=$HOME/dev/my-projects-source/public-on-github/ai/transformers/llm/train-bert-on-yelp
  cd $ML_PROJECT
  
  python ./misc/test-rocm.py
#+END_SRC

#+RESULTS:

Great. PyTorch works with ROCm on an old Ryzen 5 2500U!! This concludes our endeavor. 



* Script to fully uninstall and reinstall ROCm (amdgpu) and PyTorch

----
sudo zypper addrepo https://download.opensuse.org/repositories/devel:languages:perl/SLE_15_SP5/devel:languages:perl.repo
sudo zypper install kernel-default-devel
# See prerequisites. Adding current user to Video and Render groups
sudo usermod -a -G render,video $LOGNAME
sudo zypper --no-gpg-checks install https://repo.radeon.com/amdgpu-install/6.0.2/sle/15.5/amdgpu-install-6.0.60002-1.noarch.rpm
sudo zypper refresh
sudo zypper install amdgpu-dkms
sudo zypper install rocm
echo "Please reboot system for all settings to take effect."
----


sudo amdgpu-install --uninstall
sudo zypper removerepo "amdgpu"
sudo zypper removerepo "amdgpu-src"
sudo zypper removerepo "amdgpu-proprietary"
sudo zypper removerepo "rocm"
sudo zypper removerepo "devel_languages_perl"

# Register repo for kernel module driver packages
sudo tee /etc/zypp/repos.d/amdgpu.repo <<EOF
[amdgpu]
name=amdgpu
baseurl=https://repo.radeon.com/amdgpu/6.0.2/sle/15.5/main/x86_64/
enabled=1
gpgcheck=1
gpgkey=https://repo.radeon.com/rocm/rocm.gpg.key
EOF

sudo zypper ref

# Register repo for ROCm packages
sudo tee --append /etc/zypp/repos.d/rocm.repo <<EOF
[ROCm-6.0.2]
name=ROCm6.0.2
baseurl=https://repo.radeon.com/rocm/zyp/6.0.2/main
enabled=1
gpgcheck=1
gpgkey=https://repo.radeon.com/rocm/rocm.gpg.key
EOF

sudo zypper ref

# The amdgpu-install version AMDGPU_ROCM_RPM amdgpu-install-6.0.60002 is version of ROCm
# The ROCm version must match the version in ROCM_TORCH_WHL - rocm6.0
# The "cp310" in ROCM_TORCH_WHL describes Python 3.10 
AMDGPU_ROCM_RPM=https://repo.radeon.com/amdgpu-install/6.0.2/sle/15.5/amdgpu-install-6.0.60002-1.noarch.rpm

# Install ROCm using amd install.
# Should work now when packages added.
# sudo zypper --no-gpg-checks install $AMDGPU_ROCM_RPM
# does not work : sudo amdgpu-install --usecase=graphics,rocm

# Install ROCM from native package manager
# sudo zypper addrepo https://download.opensuse.org/repositories/devel:languages:perl/SLE_15_SP5/devel:languages:perl.repo
sudo zypper addrepo https://download.opensuse.org/repositories/devel:/languages:/perl/openSUSE_Tumbleweed/
sudo zypper install kernel-default-devel
# See prerequisites. Adding current user to Video and Render groups
sudo usermod -a -G render,video $LOGNAME
sudo zypper --no-gpg-checks install $AMDGPU_ROCM_RPM
sudo zypper refresh
sudo zypper install amdgpu-dkms

echo "Please reboot system for all settings to take effect."
sudo zypper install rocm

echo "Please reboot system for all settings to take effect."

read

# Make sure to use 3.10
ML_PROJECT=$HOME/dev/my-projects-source/public-on-github/ai/transformers/llm/train-bert-on-yelp
cd $ML_PROJECT

# This is failing in Tumbleweed
# ROCM_TORCH_WHL=https://repo.radeon.com/rocm/manylinux/rocm-rel-6.0.2/torch-2.1.2+rocm6.0-cp310-cp310-linux_x86_64.whl

# Alternative for ROCM_TORCH_WHL is to specify nithgtly torch builds; they have
# WHL files for rocm6.0-cp310. Those seem to be automatically selected when we specify simply:
ROCM_TORCH_WHL="--index-url https://download.pytorch.org/whl/nightly/rocm6.0"

# Install Torch from nightly wheel files.
cd HUGE-NO-BACKUP
pip3 install --force-reinstall --pre torch torchvision torchaudio $ROCM_TORCH_WHL




============================


After the above installation of ROCm (amdgpu) todo check if it worked and how


AFTER rocm install:
localhost:~ # glxinfo | egrep -i 'device|memory'
    Device: llvmpipe (LLVM 15.0.7, 256 bits) (0xffffffff)
    Video memory: 14875MB
    Unified memory: yes
    GL_AMD_multi_draw_indirect, GL_AMD_pinned_memory, 
    GL_EXT_framebuffer_object, GL_EXT_framebuffer_sRGB, GL_EXT_memory_object, 
    GL_EXT_memory_object_fd, GL_EXT_packed_depth_stencil, GL_EXT_packed_float, 
    GL_AMD_pinned_memory, GL_AMD_query_buffer_object, 
    GL_EXT_gpu_program_parameters, GL_EXT_gpu_shader4, GL_EXT_memory_object, 
    GL_EXT_memory_object_fd, GL_EXT_multi_draw_arrays, 
    GL_EXT_map_buffer_range, GL_EXT_memory_object, GL_EXT_memory_object_fd, 


> rocminfo | grep Name

#> dmesg | grep kfd

#> dmesg | grep rocm

#> sudo modprobe -vvr amdgpu

#> mzimmermann@localhost:~/tmp> sudo modprobe -vv amdgpu

AFTER REBOOT, rocm and amdgpu loaded:

localhost:~ #
  Name:                    gfx902                             
      Name:                    amdgcn-amd-amdhsa--gfx902:xnack+   



============ NO LUCK - TRY REBUILD PYTORCH FROM GIT AS SUGGESTED HERE

https://github.com/ROCm/ROCm/issues/1743

Here is a workaround to run pytorch on gfx90c.
Just build pytorch for gfx900 and override gfx90c to gfx900.

# Build pytorch
git clone https://github.com/pytorch/pytorch.git  
cd pytorch  
git submodule update --init --recursive
pip3 install -r requirements.txt
pip3 install enum34 numpy pyyaml setuptools typing cffi future hypothesis typing_extensions
python3 tools/amd_build/build_amd.py
PYTORCH_ROCM_ARCH=gfx900 USE_ROCM=1 MAX_JOBS=4 python3 setup.py install

# Run an example
git clone https://github.com/pytorch/examples.git
cd examples/mnist
pip3 install -r requirements.txt
HSA_OVERRIDE_GFX_VERSION=9.0.0 python3 main.py

 the above failed *THIS BUILD OK BUT FAILED - HSA_STATUS_ERROR_MEMORY_APERTURE_VIOLATION*

AMD_LOG_LEVEL=5 HSA_OVERRIDE_GFX_VERSION=9.0.0 python


========================= FAILED - TRY TO DISABLE SOME THINGS IN HARDWARE GPU

According to https://github.com/ROCm/ROCm/issues/1743#issuecomment-1149902796
sudo modprobe amdgpu ppfeaturemask=0xfff73fff


======================== SET MORE UMA/VRAM MEMORY

sEE https://bbs.archlinux.org/viewtopic.php?id=283308
